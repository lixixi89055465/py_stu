{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m3GReUeOBJW"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None\n",
        "\n",
        "logging = logging.getLogger(__name__)\n",
        "global_config = None\n",
        "\n",
        "dataset_name = 'lamini_docs.jsonl'\n",
        "dataset_path = f\"/content/{dataset_name}\"\n",
        "use_hf = False\n",
        "\n",
        "dataset_path = 'lamini/lamini_docs'\n",
        "use_hf = True  # 是否使用huggingface\n",
        "model_name = 'EleutherAI/pythia-70m'\n",
        "training_config = {\n",
        "    'model': {\n",
        "        'pretrained_name': model_name,\n",
        "        'max_length': 2048\n",
        "    },\n",
        "    'datasets': {\n",
        "        'use_hf': use_hf,\n",
        "        'path': dataset_path\n",
        "    },\n",
        "    'verbose': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "APPMjqGdOqwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "klYDHTDOPPC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_count=torch.cuda.device_count()\n",
        "if device_count>0:\n",
        "  logger.debug('Select GPU device')\n",
        "  device=torch.device('cuda')\n",
        "else:\n",
        "  logger.debug('Select CPU device')\n",
        "  device=torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "N3gUxgwhPXE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "id": "IJk5WGGBPhg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text,model,tokenizer,max_input_tokens=1000,max_output_tokens=100):\n",
        "  #Tokenize\n",
        "  input_ids=tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors='pt',\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        "  )\n",
        "  #Generate\n",
        "  device=model.device\n",
        "  generated_tokens_with_prompt=model.generate(\n",
        "      input_ids=input_ids.to(device),\n",
        "      max_length=max_output_tokens\n",
        "  )\n",
        "  #Decode\n",
        "  generated_text_with_prompt=tokenizer.batch_decode(generated_tokens_with_prompt,skip_special_tokens=True)\n",
        "  #Strip the prompt\n",
        "  generated_text_answer=generated_text_with_prompt[0][len(text):]\n",
        "  return generated_text_answer\n"
      ],
      "metadata": {
        "id": "BGCPIUDNPxZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text=test_data[0]['question']\n",
        "print('Question input (test): ',test_text)\n",
        "print(f'Correct answer from Lamini docs:{test_dataset[0]['answer']}')\n",
        "print(\"model's answer:\")\n",
        "print(inference(text_text,base_model,tokenizer))"
      ],
      "metadata": {
        "id": "SrbC0P0tQTc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps=3\n",
        "trained_model_name=f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir=trained_model_name"
      ],
      "metadata": {
        "id": "GkNK2aZbTr-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args=TrainingArguments(\n",
        "      # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=1,\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "        # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "AiAjbXXCT07V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}