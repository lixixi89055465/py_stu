# -*- coding: utf-8 -*-
"""data_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HTaozPVlf-vO4BvnU4q0d4uaRzqf58ul
"""

# !pip install datasets

import pandas as pd
import datasets
from pprint import pprint
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-70m')
text='hi,how are you'
encoded_text=tokenizer(text)['input_ids']

encoded_text

tokenizer(text)['input_ids']

decoded_text=tokenizer.decode(encoded_text)
print('Decoded tokens back into text:',decoded_text)

list_texts=['Hi,how are you?',"I'm good","yes"]
encoded_text=tokenizer(list_texts)
print('Encoded several texts:',encoded_text['input_ids'])

tokenizer.pad_token=tokenizer.eos_token
encoded_texts_longest=tokenizer(list_texts,padding=True)
print('Using padding:',encoded_texts_longest['input_ids'])

encoded_texts_truncation=tokenizer(list_texts,max_length=3,truncation=True)
print('Using truncation :',encoded_texts_truncation['input_ids'])

tokenizer.truncation_side='left'
encoded_texts_truncation_left=tokenizer(list_texts,max_length=3,truncation=True)
print('Using left_side truncation :',encoded_texts_truncation_left['input_ids'])

encoded_text_both=tokenizer(list_texts,max_length=3,truncation=True,padding=True)
print('Using both padding and truncation:',encoded_text_both['input_ids'])

import pandas as pd
filename='./lamini_docs.jsonl'
instruction_dataset_df = pd.read_json(filename,lines=True)
examples=instruction_dataset_df.to_dict()
if 'question' in examples and 'answer' in examples:
  text=examples['question'][0] + examples['answer'][0]
elif 'instruction' in examples and 'response' in examples:
  text=examples['question'][0]+examples['answer'][0]
elif 'input' in examples and 'output' in examples:
  text=examples['input'][0]+examples['output'][0]
else:
  text=examples['text'][0]
prompt_template='''###question:
{question}
### Answer:'''

num_examples=len(examples['question'])
finetuning_dataset=[]
for i in range(num_examples):
  question=examples['question'][i]
  answer=examples['answer'][i]
  text_with_prompt_template=prompt_template.format(question=question)
  finetuning_dataset.append({'question':text_with_prompt_template,'answer':answer})
from pprint import pprint
print('One datapoint in the fineturning dataset:')
pprint(finetuning_dataset[0])

text=finetuning_dataset[0]['question']+finetuning_dataset[0]['answer']
tokenized_inputs=tokenizer(
    text,
    return_tensors='np',
    padding=True
)
print(tokenized_inputs['input_ids'])

max_length=2048
max_length=min(
    tokenized_inputs['input_ids'].shape[1],
    max_length,
)

tokenized_inputs=tokenizer(
    text,
    return_tensors='np',
    truncation=True,
    max_length=max_length
)

tokenized_inputs['input_ids']

def tokenize_function(examples):
  if 'question' in examples and 'answer' in examples:
    text=examples['question'][0]+examples['answer'][0]
  elif 'input' in examples and 'output' in examples:
    text=examples['input'][0]+examples['output'][0]
  else:
    text=examples['text'][0]
  tokenizer.pad_token=tokenizer.eos_token
  tokenized_inputs=tokenizer(
      text,
      return_tensors='np',
      padding=True
  )
  max_length=min(
      tokenized_inputs['input_ids'].shape[1],
      2048
  )
  tokenizer.truncation_side='left'
  tokenized_inputs=tokenizer(
      text,
      return_tensors='np',
      truncation=True,
      max_length=max_length
  )
  return tokenized_inputs

finetuning_dataset_loaded=datasets.load_dataset('json',data_files=filename)['train']

tokenized_dataset=finetuning_dataset_loaded.map(
    tokenize_function,
    batched=True,
    batch_size=1,
    drop_last_batch=True
)
print(tokenized_dataset)

tokenized_dataset=tokenized_dataset.add_column('labels',tokenized_dataset['input_ids'])

split_dataset=tokenized_dataset.train_test_split(test_size=0.1,shuffle=True,seed=123)
print(split_dataset)

finetuning_dataset_path='lamini/lamini_docs'
finetuning_dataset=datasets.load_dataset(finetuning_dataset_path)
print(finetuning_dataset)

taylor_swift_dataset='lamini/taylor_swift'
bts_dataset='lamini/bts'
open_llms='lamini/open_llms'

dataset_swiftie=datasets.load_dataset(taylor_swift_dataset)
print(dataset_swiftie['train'][1])

