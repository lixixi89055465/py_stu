{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717e1f44",
   "metadata": {
    "papermill": {
     "duration": 0.021473,
     "end_time": "2021-07-01T08:03:36.068168",
     "exception": false,
     "start_time": "2021-07-01T08:03:36.046695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**由于本地没有可用的gpu资源以及kaggle kernels gpu的时长限制，故借本次比赛的机会学习使用TPU进行训练与推理**\n",
    "\n",
    "该代码主要依据Araik Tamazian的notebook [FC Ensemble External Data (EffNet+DenseNet)](https://www.kaggle.com/atamazian/fc-ensemble-external-data-effnet-densenet) 进行改写\n",
    "\n",
    "同时也参考了以下内容：\n",
    "* [simple resnet baseline](https://www.kaggle.com/nekokiku/simple-resnet-baseline) by Neko Kiku\n",
    "* [del_duplicate_image](https://github.com/seefun/TorchUtils/blob/master/torch_utils/dataset/del_duplicate_image.py) by seefun\n",
    "* [Getting started with 100+ flowers on TPU](https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu) by Martine Goerner\n",
    "* [TPU Flowers](https://www.kaggle.com/tusharkendre/tpu-flowers) by Tushar Kendre and Shreyaansh Gupta (random_blockout augmentation)\n",
    "* [TPU: ENet B7 + DenseNet](https://www.kaggle.com/wrrosa/tpu-enet-b7-densenet) by Wojtek Rosa\n",
    "\n",
    "对以上作者的无私分享表示由衷的感谢！\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d70286",
   "metadata": {
    "papermill": {
     "duration": 0.020295,
     "end_time": "2021-07-01T08:03:36.111639",
     "exception": false,
     "start_time": "2021-07-01T08:03:36.091344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "本notebook可在kaggle kernels的TPU环境下实现一键跑通，并在private test上获得0.988左右的成绩。如果大家再选择不同的模型组合，训练策略以及数据增强方法进行训练与推理，最终对多个结果采取投票的方法进行融合，成绩可达0.989-0.990+。关于投票的代码在其他小伙伴分享的notebook中已有详细的实现，这里就不再赘述了。\n",
    "\n",
    "在本次比赛中并未使用太多的数据增强方法，也还没来得及尝试沐神的ResNeSt，所以还是有很大的提升空间的，欢迎大家进行魔改，也希望大家多多分享改进方案。\n",
    "\n",
    "**<span style=\"color:blue\">第一次分享notebook，大家有什么意见尽管提；如果对大家有帮助，也请各位不要吝啬vote呀，哈哈哈</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d851547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:03:36.167249Z",
     "iopub.status.busy": "2021-07-01T08:03:36.162316Z",
     "iopub.status.idle": "2021-07-01T08:03:45.628729Z",
     "shell.execute_reply": "2021-07-01T08:03:45.627873Z",
     "shell.execute_reply.started": "2021-07-01T01:53:26.720621Z"
    },
    "papermill": {
     "duration": 9.496891,
     "end_time": "2021-07-01T08:03:45.628938",
     "exception": false,
     "start_time": "2021-07-01T08:03:36.132047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c71dc8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-01T08:03:45.678512Z",
     "iopub.status.busy": "2021-07-01T08:03:45.677789Z",
     "iopub.status.idle": "2021-07-01T08:03:53.129318Z",
     "shell.execute_reply": "2021-07-01T08:03:53.129765Z",
     "shell.execute_reply.started": "2021-07-01T01:53:35.420074Z"
    },
    "papermill": {
     "duration": 7.478829,
     "end_time": "2021-07-01T08:03:53.129997",
     "exception": false,
     "start_time": "2021-07-01T08:03:45.651168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math, re, os, random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import efficientnet.tfkeras as efn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "print(\"TF version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf053f",
   "metadata": {
    "papermill": {
     "duration": 0.020408,
     "end_time": "2021-07-01T08:03:53.170179",
     "exception": false,
     "start_time": "2021-07-01T08:03:53.149771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TPU detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09693c9b",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-07-01T08:03:53.212754Z",
     "iopub.status.busy": "2021-07-01T08:03:53.211807Z",
     "iopub.status.idle": "2021-07-01T08:03:58.652174Z",
     "shell.execute_reply": "2021-07-01T08:03:58.652683Z",
     "shell.execute_reply.started": "2021-07-01T01:53:42.608364Z"
    },
    "papermill": {
     "duration": 5.462968,
     "end_time": "2021-07-01T08:03:58.652876",
     "exception": false,
     "start_time": "2021-07-01T08:03:53.189908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accelerators:  8\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686853d",
   "metadata": {
    "papermill": {
     "duration": 0.019013,
     "end_time": "2021-07-01T08:03:58.691442",
     "exception": false,
     "start_time": "2021-07-01T08:03:58.672429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad0878f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:03:58.735741Z",
     "iopub.status.busy": "2021-07-01T08:03:58.735102Z",
     "iopub.status.idle": "2021-07-01T08:03:58.739132Z",
     "shell.execute_reply": "2021-07-01T08:03:58.738620Z",
     "shell.execute_reply.started": "2021-07-01T01:53:48.83741Z"
    },
    "papermill": {
     "duration": 0.028074,
     "end_time": "2021-07-01T08:03:58.739266",
     "exception": false,
     "start_time": "2021-07-01T08:03:58.711192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48e3a7",
   "metadata": {
    "papermill": {
     "duration": 0.018836,
     "end_time": "2021-07-01T08:03:58.777348",
     "exception": false,
     "start_time": "2021-07-01T08:03:58.758512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data access and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed7caa",
   "metadata": {
    "papermill": {
     "duration": 0.018684,
     "end_time": "2021-07-01T08:03:58.815073",
     "exception": false,
     "start_time": "2021-07-01T08:03:58.796389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TPUs read data directly from Google Cloud Storage (GCS), so we need to copy the dataset to a GCS bucket co-located with the TPU. To do that, pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. \n",
    "\n",
    "这里特别说明一下，Google要求所有在Kaggle平台上使用TPU进行模型训练的用户须将数据放在GCS上，故该notebook的数据集已公开在kaggle datasets上。考虑到测试集与训练集可能包含相同的标注噪声，该数据集选用未经清洗的原始数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b90767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:03:58.862225Z",
     "iopub.status.busy": "2021-07-01T08:03:58.861572Z",
     "iopub.status.idle": "2021-07-01T08:04:01.067972Z",
     "shell.execute_reply": "2021-07-01T08:04:01.068430Z",
     "shell.execute_reply.started": "2021-07-01T01:53:48.844365Z"
    },
    "papermill": {
     "duration": 2.234474,
     "end_time": "2021-07-01T08:04:01.068619",
     "exception": false,
     "start_time": "2021-07-01T08:03:58.834145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maclura_pomifera            353\n",
       "ulmus_rubra                 235\n",
       "prunus_virginiana           223\n",
       "acer_rubrum                 217\n",
       "broussonettia_papyrifera    214\n",
       "                           ... \n",
       "cedrus_deodara               58\n",
       "ailanthus_altissima          58\n",
       "crataegus_crus-galli         54\n",
       "evodia_daniellii             53\n",
       "juniperus_virginiana         51\n",
       "Name: label, Length: 176, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/classify-leaves/train.csv')\n",
    "CLASSES = sorted(list(set(train['label'])))\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('leavestfrec')\n",
    "\n",
    "Training_filenames, Validation_filenames = [], []\n",
    "\n",
    "for i in range(5):\n",
    "    Training_filenames.append(tf.io.gfile.glob(GCS_DS_PATH + f'/train{i}.tfrec'))\n",
    "    Validation_filenames.append(tf.io.gfile.glob(GCS_DS_PATH + f'/val{i}.tfrec'))\n",
    "\n",
    "\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/test.tfrec')\n",
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96c9a8",
   "metadata": {
    "papermill": {
     "duration": 0.019334,
     "end_time": "2021-07-01T08:04:01.108576",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.089242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random blockout augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6fe66d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:04:01.152696Z",
     "iopub.status.busy": "2021-07-01T08:04:01.151711Z",
     "iopub.status.idle": "2021-07-01T08:04:01.167301Z",
     "shell.execute_reply": "2021-07-01T08:04:01.168058Z",
     "shell.execute_reply.started": "2021-07-01T01:53:49.968496Z"
    },
    "papermill": {
     "duration": 0.040106,
     "end_time": "2021-07-01T08:04:01.168294",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.128188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_erasing(img, sl=0.1, sh=0.2, rl=0.4, p=0.3):\n",
    "    h = tf.shape(img)[0]\n",
    "    w = tf.shape(img)[1]\n",
    "    c = tf.shape(img)[2]\n",
    "    origin_area = tf.cast(h*w, tf.float32)\n",
    "\n",
    "    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n",
    "    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n",
    "\n",
    "    e_height_h = tf.minimum(e_size_h, h)\n",
    "    e_width_h = tf.minimum(e_size_h, w)\n",
    "\n",
    "    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n",
    "    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n",
    "\n",
    "    erase_height = abs(erase_height-10)\n",
    "    erase_width = abs(erase_width-10)\n",
    "    \n",
    "    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n",
    "    erase_area = tf.cast(erase_area, tf.uint8)\n",
    "\n",
    "    pad_h = h - erase_height\n",
    "    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n",
    "    pad_bottom = pad_h - pad_top\n",
    "\n",
    "    pad_w = w - erase_width\n",
    "    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n",
    "    pad_right = pad_w - pad_left\n",
    "\n",
    "    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n",
    "    erase_mask = tf.squeeze(erase_mask, axis=0)\n",
    "    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n",
    "\n",
    "    return tf.cond(tf.random.uniform([], 0, 1) > p, lambda: tf.cast(img, img.dtype), lambda:  tf.cast(erased_img, img.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffad22",
   "metadata": {
    "papermill": {
     "duration": 0.023575,
     "end_time": "2021-07-01T08:04:01.219261",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.195686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cccc2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:04:01.273938Z",
     "iopub.status.busy": "2021-07-01T08:04:01.273028Z",
     "iopub.status.idle": "2021-07-01T08:04:01.296173Z",
     "shell.execute_reply": "2021-07-01T08:04:01.296701Z",
     "shell.execute_reply.started": "2021-07-01T01:53:49.989697Z"
    },
    "papermill": {
     "duration": 0.052223,
     "end_time": "2021-07-01T08:04:01.296950",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.244727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 14683 training images, 3670 validation images, 8800 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def onehot(image,label):\n",
    "    return image,tf.one_hot(label, len(CLASSES))\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    feature  = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['label'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    feature  = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict leaves classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.5)\n",
    "    image = random_erasing(image)\n",
    "    return image, label\n",
    "\n",
    "def data_hflip(image, idnum):\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image, idnum\n",
    "\n",
    "def get_training_dataset(training_filenames, do_onehot=False):\n",
    "    dataset = load_dataset(training_filenames, labeled=True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    if do_onehot:\n",
    "        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(validation_filenames, ordered=False, do_onehot=False):\n",
    "    dataset = load_dataset(validation_filenames, labeled=True, ordered=ordered)\n",
    "    if do_onehot:\n",
    "        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False, augmented=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.map(data_hflip, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "NUM_TRAINING_IMAGES = 14683\n",
    "NUM_VALIDATION_IMAGES = 3670\n",
    "NUM_TEST_IMAGES = 8800\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "VALIDATION_STEPS = -(-NUM_VALIDATION_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\n",
    "TEST_STEPS = -(-NUM_TEST_IMAGES // BATCH_SIZE)             # The \"-(-//)\" trick rounds up instead of down :-)\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74512375",
   "metadata": {
    "papermill": {
     "duration": 0.021571,
     "end_time": "2021-07-01T08:04:01.340268",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.318697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e177a43",
   "metadata": {
    "papermill": {
     "duration": 0.021165,
     "end_time": "2021-07-01T08:04:01.383296",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.362131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbeef6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:04:01.430307Z",
     "iopub.status.busy": "2021-07-01T08:04:01.429636Z",
     "iopub.status.idle": "2021-07-01T08:04:01.655916Z",
     "shell.execute_reply": "2021-07-01T08:04:01.655295Z",
     "shell.execute_reply.started": "2021-07-01T01:53:50.017371Z"
    },
    "papermill": {
     "duration": 0.251098,
     "end_time": "2021-07-01T08:04:01.656069",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.404971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 1e-05 to 0.0004 to 1.04e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "\n",
    "LR_START = 0.00001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 9\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_EXP_DECAY = .87\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
    "\n",
    "rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1beb0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:04:01.709258Z",
     "iopub.status.busy": "2021-07-01T08:04:01.708570Z",
     "iopub.status.idle": "2021-07-01T08:04:01.712051Z",
     "shell.execute_reply": "2021-07-01T08:04:01.712513Z",
     "shell.execute_reply.started": "2021-07-01T01:53:50.228906Z"
    },
    "papermill": {
     "duration": 0.033131,
     "end_time": "2021-07-01T08:04:01.712687",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.679556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(base_model):\n",
    "    base_model.trainable = True\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='nadam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "#         metrics=[tfa.metrics.F1Score(len(CLASSES), average='macro')],\n",
    "        metrics=['accuracy'],\n",
    "        # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n",
    "        # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n",
    "        steps_per_execution=16\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74dfb3",
   "metadata": {
    "papermill": {
     "duration": 0.021439,
     "end_time": "2021-07-01T08:04:01.756080",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.734641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## EfficientNet B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eee2d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:04:01.811669Z",
     "iopub.status.busy": "2021-07-01T08:04:01.810876Z",
     "iopub.status.idle": "2021-07-01T10:35:54.748128Z",
     "shell.execute_reply": "2021-07-01T10:35:54.746735Z",
     "shell.execute_reply.started": "2021-07-01T01:53:50.235763Z"
    },
    "papermill": {
     "duration": 9112.970693,
     "end_time": "2021-07-01T10:35:54.748651",
     "exception": false,
     "start_time": "2021-07-01T08:04:01.777958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the fold0 !\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 4s 0us/step\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 426s - loss: 5.1841 - accuracy: 0.0090 - val_loss: 5.1717 - val_accuracy: 0.0117\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.01171, saving model to effnetb7_0_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 21s - loss: 5.0765 - accuracy: 0.0327 - val_loss: 4.8833 - val_accuracy: 0.1177\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.01171 to 0.11768, saving model to effnetb7_0_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 21s - loss: 4.0019 - accuracy: 0.1810 - val_loss: 2.7220 - val_accuracy: 0.3827\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.11768 to 0.38273, saving model to effnetb7_0_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 21s - loss: 2.3815 - accuracy: 0.4186 - val_loss: 1.3558 - val_accuracy: 0.6609\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38273 to 0.66086, saving model to effnetb7_0_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 21s - loss: 1.5754 - accuracy: 0.5833 - val_loss: 0.7761 - val_accuracy: 0.7902\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.66086 to 0.79025, saving model to effnetb7_0_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 21s - loss: 1.1421 - accuracy: 0.6927 - val_loss: 0.5278 - val_accuracy: 0.8515\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.79025 to 0.85154, saving model to effnetb7_0_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 21s - loss: 0.8478 - accuracy: 0.7681 - val_loss: 0.3503 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.85154 to 0.89186, saving model to effnetb7_0_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 21s - loss: 0.6578 - accuracy: 0.8156 - val_loss: 0.2722 - val_accuracy: 0.9093\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.89186 to 0.90929, saving model to effnetb7_0_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 21s - loss: 0.5262 - accuracy: 0.8502 - val_loss: 0.2115 - val_accuracy: 0.9294\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.90929 to 0.92945, saving model to effnetb7_0_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 21s - loss: 0.4690 - accuracy: 0.8690 - val_loss: 0.1995 - val_accuracy: 0.9308\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.92945 to 0.93081, saving model to effnetb7_0_best.h5\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 21s - loss: 0.3499 - accuracy: 0.8984 - val_loss: 0.1796 - val_accuracy: 0.9403\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.93081 to 0.94034, saving model to effnetb7_0_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 21s - loss: 0.2974 - accuracy: 0.9138 - val_loss: 0.1380 - val_accuracy: 0.9512\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.94034 to 0.95124, saving model to effnetb7_0_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 21s - loss: 0.2483 - accuracy: 0.9278 - val_loss: 0.1365 - val_accuracy: 0.9521\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95124 to 0.95206, saving model to effnetb7_0_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 21s - loss: 0.2220 - accuracy: 0.9342 - val_loss: 0.1042 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.95206 to 0.96431, saving model to effnetb7_0_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 21s - loss: 0.1792 - accuracy: 0.9478 - val_loss: 0.0928 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.96431\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 21s - loss: 0.1730 - accuracy: 0.9482 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96431 to 0.96895, saving model to effnetb7_0_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 21s - loss: 0.1544 - accuracy: 0.9562 - val_loss: 0.0952 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96895 to 0.97031, saving model to effnetb7_0_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 21s - loss: 0.1428 - accuracy: 0.9580 - val_loss: 0.0910 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.97031 to 0.97140, saving model to effnetb7_0_best.h5\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 21s - loss: 0.1309 - accuracy: 0.9623 - val_loss: 0.0935 - val_accuracy: 0.9676\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97140\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 21s - loss: 0.1288 - accuracy: 0.9619 - val_loss: 0.0878 - val_accuracy: 0.9687\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97140\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 21s - loss: 0.1250 - accuracy: 0.9627 - val_loss: 0.0811 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.97140 to 0.97249, saving model to effnetb7_0_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 21s - loss: 0.1196 - accuracy: 0.9624 - val_loss: 0.0756 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.97249 to 0.97303, saving model to effnetb7_0_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 21s - loss: 0.1109 - accuracy: 0.9655 - val_loss: 0.0767 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97303 to 0.97494, saving model to effnetb7_0_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 21s - loss: 0.1062 - accuracy: 0.9679 - val_loss: 0.0730 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.97494 to 0.97630, saving model to effnetb7_0_best.h5\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 21s - loss: 0.1070 - accuracy: 0.9661 - val_loss: 0.0724 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.97630\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 21s - loss: 0.0985 - accuracy: 0.9692 - val_loss: 0.0717 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97630\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 21s - loss: 0.0985 - accuracy: 0.9686 - val_loss: 0.0703 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97630\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 21s - loss: 0.0999 - accuracy: 0.9691 - val_loss: 0.0715 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97630\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 21s - loss: 0.0947 - accuracy: 0.9705 - val_loss: 0.0705 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97630\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 21s - loss: 0.0944 - accuracy: 0.9712 - val_loss: 0.0685 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97630\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 21s - loss: 0.0926 - accuracy: 0.9705 - val_loss: 0.0703 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97630\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 21s - loss: 0.0914 - accuracy: 0.9713 - val_loss: 0.0697 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.97630 to 0.97685, saving model to effnetb7_0_best.h5\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 21s - loss: 0.1008 - accuracy: 0.9670 - val_loss: 0.0672 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97685\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 21s - loss: 0.0930 - accuracy: 0.9716 - val_loss: 0.0664 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.97685 to 0.97766, saving model to effnetb7_0_best.h5\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 21s - loss: 0.0868 - accuracy: 0.9727 - val_loss: 0.0667 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97766\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 21s - loss: 0.0837 - accuracy: 0.9716 - val_loss: 0.0659 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97766\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 21s - loss: 0.0841 - accuracy: 0.9746 - val_loss: 0.0671 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97766\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 21s - loss: 0.0818 - accuracy: 0.9735 - val_loss: 0.0663 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97766\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 21s - loss: 0.0848 - accuracy: 0.9744 - val_loss: 0.0660 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97766\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 21s - loss: 0.0805 - accuracy: 0.9746 - val_loss: 0.0682 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97766\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 21s - loss: 0.0806 - accuracy: 0.9745 - val_loss: 0.0672 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97766\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 21s - loss: 0.0852 - accuracy: 0.9733 - val_loss: 0.0669 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97766\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 21s - loss: 0.0850 - accuracy: 0.9733 - val_loss: 0.0675 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00043: val_accuracy improved from 0.97766 to 0.97794, saving model to effnetb7_0_best.h5\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 21s - loss: 0.0878 - accuracy: 0.9726 - val_loss: 0.0678 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97794\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 21s - loss: 0.0837 - accuracy: 0.9726 - val_loss: 0.0674 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97794\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 21s - loss: 0.0800 - accuracy: 0.9752 - val_loss: 0.0680 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97794\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 21s - loss: 0.0833 - accuracy: 0.9731 - val_loss: 0.0674 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97794\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 21s - loss: 0.0788 - accuracy: 0.9750 - val_loss: 0.0670 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97794\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 21s - loss: 0.0843 - accuracy: 0.9732 - val_loss: 0.0667 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97794\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 21s - loss: 0.0730 - accuracy: 0.9768 - val_loss: 0.0678 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97794\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 21s - loss: 0.0750 - accuracy: 0.9766 - val_loss: 0.0711 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97794\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 21s - loss: 0.0774 - accuracy: 0.9742 - val_loss: 0.0687 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97794\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 21s - loss: 0.0773 - accuracy: 0.9744 - val_loss: 0.0678 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97794\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 21s - loss: 0.0679 - accuracy: 0.9783 - val_loss: 0.0682 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97794\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 21s - loss: 0.0761 - accuracy: 0.9742 - val_loss: 0.0681 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97794\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 21s - loss: 0.0754 - accuracy: 0.9761 - val_loss: 0.0672 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97794\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 21s - loss: 0.0765 - accuracy: 0.9756 - val_loss: 0.0668 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97794\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 21s - loss: 0.0716 - accuracy: 0.9770 - val_loss: 0.0675 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97794\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 21s - loss: 0.0696 - accuracy: 0.9758 - val_loss: 0.0673 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97794\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 21s - loss: 0.0771 - accuracy: 0.9742 - val_loss: 0.0677 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97794\n",
      "Start training the fold1 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 440s - loss: 5.1855 - accuracy: 0.0058 - val_loss: 5.1615 - val_accuracy: 0.0076\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00763, saving model to effnetb7_1_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 21s - loss: 5.0833 - accuracy: 0.0323 - val_loss: 4.9128 - val_accuracy: 0.1019\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.00763 to 0.10188, saving model to effnetb7_1_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 21s - loss: 3.9731 - accuracy: 0.1827 - val_loss: 2.5620 - val_accuracy: 0.4056\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.10188 to 0.40561, saving model to effnetb7_1_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 21s - loss: 2.3709 - accuracy: 0.4230 - val_loss: 1.3555 - val_accuracy: 0.6331\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.40561 to 0.63307, saving model to effnetb7_1_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 21s - loss: 1.5601 - accuracy: 0.5914 - val_loss: 0.7929 - val_accuracy: 0.7794\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.63307 to 0.77935, saving model to effnetb7_1_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 21s - loss: 1.1018 - accuracy: 0.7039 - val_loss: 0.4741 - val_accuracy: 0.8630\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.77935 to 0.86298, saving model to effnetb7_1_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 21s - loss: 0.8276 - accuracy: 0.7688 - val_loss: 0.3538 - val_accuracy: 0.8899\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.86298 to 0.88995, saving model to effnetb7_1_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 21s - loss: 0.6569 - accuracy: 0.8153 - val_loss: 0.2576 - val_accuracy: 0.9150\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.88995 to 0.91501, saving model to effnetb7_1_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 21s - loss: 0.5344 - accuracy: 0.8488 - val_loss: 0.2269 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.91501 to 0.92509, saving model to effnetb7_1_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 21s - loss: 0.4406 - accuracy: 0.8732 - val_loss: 0.1895 - val_accuracy: 0.9373\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.92509 to 0.93735, saving model to effnetb7_1_best.h5\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 21s - loss: 0.3541 - accuracy: 0.8982 - val_loss: 0.1642 - val_accuracy: 0.9482\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.93735 to 0.94824, saving model to effnetb7_1_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 21s - loss: 0.2955 - accuracy: 0.9125 - val_loss: 0.1237 - val_accuracy: 0.9575\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.94824 to 0.95750, saving model to effnetb7_1_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 21s - loss: 0.2422 - accuracy: 0.9291 - val_loss: 0.0999 - val_accuracy: 0.9679\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95750 to 0.96786, saving model to effnetb7_1_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 21s - loss: 0.2040 - accuracy: 0.9403 - val_loss: 0.0843 - val_accuracy: 0.9687\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.96786 to 0.96867, saving model to effnetb7_1_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 21s - loss: 0.1820 - accuracy: 0.9455 - val_loss: 0.0745 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.96867 to 0.97140, saving model to effnetb7_1_best.h5\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 21s - loss: 0.1672 - accuracy: 0.9511 - val_loss: 0.0850 - val_accuracy: 0.9698\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.97140\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 21s - loss: 0.1581 - accuracy: 0.9541 - val_loss: 0.0770 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.97140\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 21s - loss: 0.1416 - accuracy: 0.9568 - val_loss: 0.0705 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97140\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 21s - loss: 0.1340 - accuracy: 0.9584 - val_loss: 0.0706 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.97140 to 0.97194, saving model to effnetb7_1_best.h5\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 21s - loss: 0.1350 - accuracy: 0.9589 - val_loss: 0.0708 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97194\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 21s - loss: 0.1214 - accuracy: 0.9629 - val_loss: 0.0716 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.97194 to 0.97412, saving model to effnetb7_1_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 21s - loss: 0.1175 - accuracy: 0.9655 - val_loss: 0.0651 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.97412\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 21s - loss: 0.1141 - accuracy: 0.9663 - val_loss: 0.0662 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97412 to 0.97521, saving model to effnetb7_1_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 21s - loss: 0.1050 - accuracy: 0.9675 - val_loss: 0.0657 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97521\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 21s - loss: 0.1054 - accuracy: 0.9663 - val_loss: 0.0635 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97521 to 0.97603, saving model to effnetb7_1_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 21s - loss: 0.1040 - accuracy: 0.9683 - val_loss: 0.0667 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97603 to 0.97657, saving model to effnetb7_1_best.h5\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 21s - loss: 0.0972 - accuracy: 0.9703 - val_loss: 0.0621 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97657\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 21s - loss: 0.0955 - accuracy: 0.9707 - val_loss: 0.0637 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97657\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 21s - loss: 0.0984 - accuracy: 0.9691 - val_loss: 0.0626 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.97657 to 0.97712, saving model to effnetb7_1_best.h5\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 21s - loss: 0.0918 - accuracy: 0.9703 - val_loss: 0.0637 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97712\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 21s - loss: 0.0880 - accuracy: 0.9725 - val_loss: 0.0635 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97712\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 21s - loss: 0.0908 - accuracy: 0.9699 - val_loss: 0.0622 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.97712 to 0.97739, saving model to effnetb7_1_best.h5\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 21s - loss: 0.0832 - accuracy: 0.9733 - val_loss: 0.0598 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00033: val_accuracy improved from 0.97739 to 0.97766, saving model to effnetb7_1_best.h5\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 21s - loss: 0.0909 - accuracy: 0.9716 - val_loss: 0.0607 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97766\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 21s - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.0616 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97766\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 21s - loss: 0.0833 - accuracy: 0.9748 - val_loss: 0.0594 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97766\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 21s - loss: 0.0808 - accuracy: 0.9746 - val_loss: 0.0587 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97766\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 21s - loss: 0.0854 - accuracy: 0.9745 - val_loss: 0.0586 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97766\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 21s - loss: 0.0809 - accuracy: 0.9742 - val_loss: 0.0596 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97766\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 21s - loss: 0.0836 - accuracy: 0.9732 - val_loss: 0.0596 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97766\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 21s - loss: 0.0833 - accuracy: 0.9738 - val_loss: 0.0587 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97766\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 21s - loss: 0.0861 - accuracy: 0.9725 - val_loss: 0.0589 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97766\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 21s - loss: 0.0786 - accuracy: 0.9727 - val_loss: 0.0575 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97766\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 21s - loss: 0.0817 - accuracy: 0.9730 - val_loss: 0.0583 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97766\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 21s - loss: 0.0745 - accuracy: 0.9755 - val_loss: 0.0584 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97766\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 21s - loss: 0.0816 - accuracy: 0.9742 - val_loss: 0.0597 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97766\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 21s - loss: 0.0817 - accuracy: 0.9755 - val_loss: 0.0588 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97766\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 21s - loss: 0.0792 - accuracy: 0.9752 - val_loss: 0.0591 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97766\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 21s - loss: 0.0781 - accuracy: 0.9754 - val_loss: 0.0586 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97766\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 21s - loss: 0.0781 - accuracy: 0.9759 - val_loss: 0.0586 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97766\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 21s - loss: 0.0731 - accuracy: 0.9751 - val_loss: 0.0582 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97766\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 21s - loss: 0.0759 - accuracy: 0.9758 - val_loss: 0.0590 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97766\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 21s - loss: 0.0822 - accuracy: 0.9727 - val_loss: 0.0614 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97766\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 21s - loss: 0.0776 - accuracy: 0.9751 - val_loss: 0.0593 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97766\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 21s - loss: 0.0793 - accuracy: 0.9745 - val_loss: 0.0596 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97766\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 21s - loss: 0.0730 - accuracy: 0.9751 - val_loss: 0.0605 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97766\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 21s - loss: 0.0757 - accuracy: 0.9765 - val_loss: 0.0587 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97766\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 21s - loss: 0.0784 - accuracy: 0.9744 - val_loss: 0.0593 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97766\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 21s - loss: 0.0742 - accuracy: 0.9766 - val_loss: 0.0590 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97766\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 21s - loss: 0.0753 - accuracy: 0.9751 - val_loss: 0.0590 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97766\n",
      "Start training the fold2 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 448s - loss: 5.1897 - accuracy: 0.0062 - val_loss: 5.1598 - val_accuracy: 0.0063\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00627, saving model to effnetb7_2_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 21s - loss: 5.0805 - accuracy: 0.0339 - val_loss: 4.8941 - val_accuracy: 0.1098\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.00627 to 0.10978, saving model to effnetb7_2_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 21s - loss: 4.0066 - accuracy: 0.1786 - val_loss: 2.5914 - val_accuracy: 0.4032\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.10978 to 0.40316, saving model to effnetb7_2_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 21s - loss: 2.3645 - accuracy: 0.4272 - val_loss: 1.3725 - val_accuracy: 0.6402\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.40316 to 0.64015, saving model to effnetb7_2_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 21s - loss: 1.5656 - accuracy: 0.5908 - val_loss: 0.7839 - val_accuracy: 0.7905\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.64015 to 0.79052, saving model to effnetb7_2_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 21s - loss: 1.0977 - accuracy: 0.7024 - val_loss: 0.4833 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.79052 to 0.86189, saving model to effnetb7_2_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 21s - loss: 0.8334 - accuracy: 0.7710 - val_loss: 0.3458 - val_accuracy: 0.8965\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.86189 to 0.89649, saving model to effnetb7_2_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 21s - loss: 0.6411 - accuracy: 0.8183 - val_loss: 0.2718 - val_accuracy: 0.9126\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.89649 to 0.91256, saving model to effnetb7_2_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 21s - loss: 0.5186 - accuracy: 0.8509 - val_loss: 0.2272 - val_accuracy: 0.9273\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.91256 to 0.92727, saving model to effnetb7_2_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 21s - loss: 0.4357 - accuracy: 0.8758 - val_loss: 0.1966 - val_accuracy: 0.9384\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.92727 to 0.93844, saving model to effnetb7_2_best.h5\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 21s - loss: 0.3428 - accuracy: 0.9010 - val_loss: 0.1541 - val_accuracy: 0.9496\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.93844 to 0.94960, saving model to effnetb7_2_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 21s - loss: 0.2819 - accuracy: 0.9165 - val_loss: 0.1269 - val_accuracy: 0.9551\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.94960 to 0.95505, saving model to effnetb7_2_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 21s - loss: 0.2391 - accuracy: 0.9288 - val_loss: 0.1047 - val_accuracy: 0.9657\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95505 to 0.96568, saving model to effnetb7_2_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 21s - loss: 0.1983 - accuracy: 0.9417 - val_loss: 0.0934 - val_accuracy: 0.9673\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.96568 to 0.96731, saving model to effnetb7_2_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 21s - loss: 0.1852 - accuracy: 0.9463 - val_loss: 0.0965 - val_accuracy: 0.9640\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.96731\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 21s - loss: 0.1611 - accuracy: 0.9523 - val_loss: 0.0807 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96731 to 0.96895, saving model to effnetb7_2_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 21s - loss: 0.1549 - accuracy: 0.9546 - val_loss: 0.0893 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96895 to 0.96922, saving model to effnetb7_2_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 21s - loss: 0.1378 - accuracy: 0.9584 - val_loss: 0.0880 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.96922 to 0.97004, saving model to effnetb7_2_best.h5\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 21s - loss: 0.1414 - accuracy: 0.9575 - val_loss: 0.0867 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97004\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 21s - loss: 0.1272 - accuracy: 0.9609 - val_loss: 0.0847 - val_accuracy: 0.9681\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97004\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 21s - loss: 0.1203 - accuracy: 0.9633 - val_loss: 0.0831 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.97004\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 21s - loss: 0.1131 - accuracy: 0.9661 - val_loss: 0.0821 - val_accuracy: 0.9687\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.97004\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 21s - loss: 0.1115 - accuracy: 0.9662 - val_loss: 0.0775 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97004 to 0.97194, saving model to effnetb7_2_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 21s - loss: 0.1081 - accuracy: 0.9686 - val_loss: 0.0793 - val_accuracy: 0.9681\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97194\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 21s - loss: 0.1080 - accuracy: 0.9672 - val_loss: 0.0753 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97194 to 0.97221, saving model to effnetb7_2_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 21s - loss: 0.1026 - accuracy: 0.9683 - val_loss: 0.0741 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97221 to 0.97358, saving model to effnetb7_2_best.h5\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 21s - loss: 0.0989 - accuracy: 0.9690 - val_loss: 0.0748 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97358\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 21s - loss: 0.0944 - accuracy: 0.9699 - val_loss: 0.0771 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.97358 to 0.97467, saving model to effnetb7_2_best.h5\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 21s - loss: 0.0941 - accuracy: 0.9705 - val_loss: 0.0792 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97467\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 21s - loss: 0.0909 - accuracy: 0.9716 - val_loss: 0.0785 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97467\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 21s - loss: 0.0870 - accuracy: 0.9725 - val_loss: 0.0782 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97467\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 21s - loss: 0.0874 - accuracy: 0.9725 - val_loss: 0.0750 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97467\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 21s - loss: 0.0876 - accuracy: 0.9723 - val_loss: 0.0734 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97467\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 21s - loss: 0.0876 - accuracy: 0.9716 - val_loss: 0.0764 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97467\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 21s - loss: 0.0874 - accuracy: 0.9724 - val_loss: 0.0743 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97467\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 21s - loss: 0.0815 - accuracy: 0.9738 - val_loss: 0.0765 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97467\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 21s - loss: 0.0812 - accuracy: 0.9750 - val_loss: 0.0745 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97467\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 21s - loss: 0.0803 - accuracy: 0.9746 - val_loss: 0.0744 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97467\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 21s - loss: 0.0831 - accuracy: 0.9731 - val_loss: 0.0759 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97467\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 21s - loss: 0.0804 - accuracy: 0.9736 - val_loss: 0.0759 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97467\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 21s - loss: 0.0781 - accuracy: 0.9757 - val_loss: 0.0786 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97467\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 21s - loss: 0.0750 - accuracy: 0.9754 - val_loss: 0.0767 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97467\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 21s - loss: 0.0767 - accuracy: 0.9759 - val_loss: 0.0763 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97467\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 21s - loss: 0.0751 - accuracy: 0.9757 - val_loss: 0.0746 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97467\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 21s - loss: 0.0834 - accuracy: 0.9726 - val_loss: 0.0775 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97467\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 21s - loss: 0.0726 - accuracy: 0.9766 - val_loss: 0.0780 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97467\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 21s - loss: 0.0717 - accuracy: 0.9771 - val_loss: 0.0769 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97467\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 21s - loss: 0.0748 - accuracy: 0.9758 - val_loss: 0.0765 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97467\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 21s - loss: 0.0784 - accuracy: 0.9744 - val_loss: 0.0752 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97467\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 21s - loss: 0.0719 - accuracy: 0.9768 - val_loss: 0.0764 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97467\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 21s - loss: 0.0728 - accuracy: 0.9772 - val_loss: 0.0763 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97467\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 21s - loss: 0.0721 - accuracy: 0.9783 - val_loss: 0.0756 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97467\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 21s - loss: 0.0735 - accuracy: 0.9761 - val_loss: 0.0741 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97467\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 21s - loss: 0.0748 - accuracy: 0.9759 - val_loss: 0.0746 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97467\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 21s - loss: 0.0729 - accuracy: 0.9770 - val_loss: 0.0749 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97467\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 21s - loss: 0.0694 - accuracy: 0.9789 - val_loss: 0.0734 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97467\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 21s - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.0730 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97467\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 21s - loss: 0.0722 - accuracy: 0.9768 - val_loss: 0.0748 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97467\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 21s - loss: 0.0708 - accuracy: 0.9778 - val_loss: 0.0732 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97467\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 21s - loss: 0.0697 - accuracy: 0.9781 - val_loss: 0.0745 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97467\n",
      "Start training the fold3 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 445s - loss: 5.1874 - accuracy: 0.0071 - val_loss: 5.1691 - val_accuracy: 0.0041\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00409, saving model to effnetb7_3_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 21s - loss: 5.0629 - accuracy: 0.0323 - val_loss: 4.8389 - val_accuracy: 0.1071\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.00409 to 0.10708, saving model to effnetb7_3_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 21s - loss: 3.9840 - accuracy: 0.1684 - val_loss: 2.7008 - val_accuracy: 0.3597\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.10708 to 0.35967, saving model to effnetb7_3_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 21s - loss: 2.5092 - accuracy: 0.3984 - val_loss: 1.5252 - val_accuracy: 0.5984\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.35967 to 0.59837, saving model to effnetb7_3_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 21s - loss: 1.6792 - accuracy: 0.5632 - val_loss: 0.9102 - val_accuracy: 0.7499\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.59837 to 0.74986, saving model to effnetb7_3_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 21s - loss: 1.2276 - accuracy: 0.6645 - val_loss: 0.5970 - val_accuracy: 0.8093\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.74986 to 0.80926, saving model to effnetb7_3_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 21s - loss: 0.9203 - accuracy: 0.7425 - val_loss: 0.3872 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.80926 to 0.87575, saving model to effnetb7_3_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 21s - loss: 0.7298 - accuracy: 0.7916 - val_loss: 0.2779 - val_accuracy: 0.9114\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.87575 to 0.91144, saving model to effnetb7_3_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 21s - loss: 0.5688 - accuracy: 0.8388 - val_loss: 0.2327 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.91144 to 0.92125, saving model to effnetb7_3_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 21s - loss: 0.4694 - accuracy: 0.8617 - val_loss: 0.1659 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.92125 to 0.94441, saving model to effnetb7_3_best.h5\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 21s - loss: 0.3720 - accuracy: 0.8904 - val_loss: 0.1563 - val_accuracy: 0.9447\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.94441 to 0.94469, saving model to effnetb7_3_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 21s - loss: 0.3098 - accuracy: 0.9104 - val_loss: 0.1232 - val_accuracy: 0.9594\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.94469 to 0.95940, saving model to effnetb7_3_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 21s - loss: 0.2568 - accuracy: 0.9236 - val_loss: 0.1056 - val_accuracy: 0.9657\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95940 to 0.96567, saving model to effnetb7_3_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 21s - loss: 0.2166 - accuracy: 0.9356 - val_loss: 0.0995 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.96567\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 21s - loss: 0.1930 - accuracy: 0.9437 - val_loss: 0.0964 - val_accuracy: 0.9619\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.96567\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 21s - loss: 0.1854 - accuracy: 0.9463 - val_loss: 0.0869 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96567 to 0.97112, saving model to effnetb7_3_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 21s - loss: 0.1711 - accuracy: 0.9492 - val_loss: 0.0829 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.97112 to 0.97193, saving model to effnetb7_3_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 21s - loss: 0.1527 - accuracy: 0.9526 - val_loss: 0.0853 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97193\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 21s - loss: 0.1420 - accuracy: 0.9574 - val_loss: 0.0812 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97193\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 21s - loss: 0.1271 - accuracy: 0.9600 - val_loss: 0.0796 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.97193 to 0.97439, saving model to effnetb7_3_best.h5\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 21s - loss: 0.1258 - accuracy: 0.9631 - val_loss: 0.0732 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.97439\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 21s - loss: 0.1205 - accuracy: 0.9629 - val_loss: 0.0756 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.97439 to 0.97602, saving model to effnetb7_3_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 21s - loss: 0.1180 - accuracy: 0.9641 - val_loss: 0.0765 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.97602\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 21s - loss: 0.1116 - accuracy: 0.9669 - val_loss: 0.0704 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97602\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 21s - loss: 0.1150 - accuracy: 0.9636 - val_loss: 0.0709 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.97602\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 21s - loss: 0.1032 - accuracy: 0.9672 - val_loss: 0.0703 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97602\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 21s - loss: 0.1004 - accuracy: 0.9692 - val_loss: 0.0691 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97602\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 21s - loss: 0.0985 - accuracy: 0.9705 - val_loss: 0.0684 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97602\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 21s - loss: 0.0969 - accuracy: 0.9703 - val_loss: 0.0677 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97602\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 21s - loss: 0.0943 - accuracy: 0.9702 - val_loss: 0.0693 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97602\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 21s - loss: 0.0921 - accuracy: 0.9713 - val_loss: 0.0654 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.97602 to 0.97657, saving model to effnetb7_3_best.h5\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 21s - loss: 0.0930 - accuracy: 0.9701 - val_loss: 0.0657 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97657\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 21s - loss: 0.0974 - accuracy: 0.9690 - val_loss: 0.0653 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97657\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 21s - loss: 0.0959 - accuracy: 0.9696 - val_loss: 0.0649 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97657\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 21s - loss: 0.0895 - accuracy: 0.9709 - val_loss: 0.0660 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97657\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 21s - loss: 0.0921 - accuracy: 0.9698 - val_loss: 0.0656 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97657\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 21s - loss: 0.0904 - accuracy: 0.9713 - val_loss: 0.0651 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.97657 to 0.97820, saving model to effnetb7_3_best.h5\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 21s - loss: 0.0881 - accuracy: 0.9731 - val_loss: 0.0661 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97820\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 21s - loss: 0.0814 - accuracy: 0.9735 - val_loss: 0.0660 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97820\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 21s - loss: 0.0875 - accuracy: 0.9730 - val_loss: 0.0618 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97820\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 21s - loss: 0.0842 - accuracy: 0.9727 - val_loss: 0.0620 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97820\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 21s - loss: 0.0871 - accuracy: 0.9735 - val_loss: 0.0631 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97820\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 21s - loss: 0.0843 - accuracy: 0.9721 - val_loss: 0.0629 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97820\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 21s - loss: 0.0847 - accuracy: 0.9734 - val_loss: 0.0640 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97820\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 21s - loss: 0.0792 - accuracy: 0.9753 - val_loss: 0.0619 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97820\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 21s - loss: 0.0828 - accuracy: 0.9738 - val_loss: 0.0636 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97820\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 21s - loss: 0.0801 - accuracy: 0.9741 - val_loss: 0.0632 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97820\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 21s - loss: 0.0815 - accuracy: 0.9733 - val_loss: 0.0631 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97820\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 21s - loss: 0.0809 - accuracy: 0.9758 - val_loss: 0.0642 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97820\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 21s - loss: 0.0769 - accuracy: 0.9756 - val_loss: 0.0639 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97820\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 21s - loss: 0.0749 - accuracy: 0.9766 - val_loss: 0.0628 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97820\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 21s - loss: 0.0767 - accuracy: 0.9736 - val_loss: 0.0635 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97820\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 21s - loss: 0.0755 - accuracy: 0.9759 - val_loss: 0.0639 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97820\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 21s - loss: 0.0809 - accuracy: 0.9748 - val_loss: 0.0631 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97820\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 21s - loss: 0.0764 - accuracy: 0.9738 - val_loss: 0.0630 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97820\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 21s - loss: 0.0839 - accuracy: 0.9731 - val_loss: 0.0628 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97820\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 21s - loss: 0.0678 - accuracy: 0.9788 - val_loss: 0.0645 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97820\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 21s - loss: 0.0733 - accuracy: 0.9777 - val_loss: 0.0637 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97820\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 21s - loss: 0.0794 - accuracy: 0.9744 - val_loss: 0.0637 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97820\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 21s - loss: 0.0771 - accuracy: 0.9753 - val_loss: 0.0631 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97820\n",
      "Start training the fold4 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 436s - loss: 5.1890 - accuracy: 0.0064 - val_loss: 5.1736 - val_accuracy: 0.0087\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.00872, saving model to effnetb7_4_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 21s - loss: 5.0569 - accuracy: 0.0327 - val_loss: 4.8432 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.00872 to 0.10000, saving model to effnetb7_4_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 21s - loss: 3.9262 - accuracy: 0.1750 - val_loss: 2.7344 - val_accuracy: 0.3992\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.10000 to 0.39918, saving model to effnetb7_4_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 21s - loss: 2.4629 - accuracy: 0.4032 - val_loss: 1.4295 - val_accuracy: 0.6460\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.39918 to 0.64605, saving model to effnetb7_4_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 21s - loss: 1.6537 - accuracy: 0.5711 - val_loss: 0.8643 - val_accuracy: 0.7580\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.64605 to 0.75804, saving model to effnetb7_4_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 21s - loss: 1.2023 - accuracy: 0.6730 - val_loss: 0.5875 - val_accuracy: 0.8237\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.75804 to 0.82371, saving model to effnetb7_4_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 21s - loss: 0.9148 - accuracy: 0.7432 - val_loss: 0.4098 - val_accuracy: 0.8668\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.82371 to 0.86676, saving model to effnetb7_4_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 21s - loss: 0.7303 - accuracy: 0.7942 - val_loss: 0.3273 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.86676 to 0.89700, saving model to effnetb7_4_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 21s - loss: 0.6075 - accuracy: 0.8270 - val_loss: 0.2485 - val_accuracy: 0.9218\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.89700 to 0.92180, saving model to effnetb7_4_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 21s - loss: 0.4821 - accuracy: 0.8621 - val_loss: 0.2195 - val_accuracy: 0.9289\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.92180 to 0.92888, saving model to effnetb7_4_best.h5\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 21s - loss: 0.3959 - accuracy: 0.8828 - val_loss: 0.1583 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.92888 to 0.94414, saving model to effnetb7_4_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 21s - loss: 0.3018 - accuracy: 0.9152 - val_loss: 0.1208 - val_accuracy: 0.9580\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.94414 to 0.95804, saving model to effnetb7_4_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 21s - loss: 0.2602 - accuracy: 0.9241 - val_loss: 0.1127 - val_accuracy: 0.9605\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95804 to 0.96049, saving model to effnetb7_4_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 21s - loss: 0.2120 - accuracy: 0.9375 - val_loss: 0.1024 - val_accuracy: 0.9651\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.96049 to 0.96512, saving model to effnetb7_4_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 21s - loss: 0.2013 - accuracy: 0.9427 - val_loss: 0.0903 - val_accuracy: 0.9673\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.96512 to 0.96730, saving model to effnetb7_4_best.h5\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 21s - loss: 0.1760 - accuracy: 0.9475 - val_loss: 0.0830 - val_accuracy: 0.9678\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96730 to 0.96785, saving model to effnetb7_4_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 21s - loss: 0.1597 - accuracy: 0.9506 - val_loss: 0.0827 - val_accuracy: 0.9708\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96785 to 0.97084, saving model to effnetb7_4_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 21s - loss: 0.1564 - accuracy: 0.9525 - val_loss: 0.0799 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97084\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 21s - loss: 0.1464 - accuracy: 0.9551 - val_loss: 0.0710 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.97084 to 0.97275, saving model to effnetb7_4_best.h5\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 21s - loss: 0.1363 - accuracy: 0.9570 - val_loss: 0.0701 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97275\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 21s - loss: 0.1248 - accuracy: 0.9609 - val_loss: 0.0689 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.97275 to 0.97330, saving model to effnetb7_4_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 21s - loss: 0.1174 - accuracy: 0.9633 - val_loss: 0.0660 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.97330 to 0.97493, saving model to effnetb7_4_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 21s - loss: 0.1138 - accuracy: 0.9645 - val_loss: 0.0629 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97493 to 0.97738, saving model to effnetb7_4_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 21s - loss: 0.1217 - accuracy: 0.9631 - val_loss: 0.0629 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97738\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 21s - loss: 0.1074 - accuracy: 0.9654 - val_loss: 0.0612 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97738 to 0.97793, saving model to effnetb7_4_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 21s - loss: 0.1101 - accuracy: 0.9672 - val_loss: 0.0637 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97793\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 21s - loss: 0.1030 - accuracy: 0.9677 - val_loss: 0.0642 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97793\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 21s - loss: 0.1066 - accuracy: 0.9682 - val_loss: 0.0626 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97793\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 21s - loss: 0.1020 - accuracy: 0.9683 - val_loss: 0.0646 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97793\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 21s - loss: 0.0970 - accuracy: 0.9700 - val_loss: 0.0631 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97793\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 21s - loss: 0.0942 - accuracy: 0.9703 - val_loss: 0.0616 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97793\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 21s - loss: 0.0991 - accuracy: 0.9698 - val_loss: 0.0615 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97793\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 21s - loss: 0.0920 - accuracy: 0.9712 - val_loss: 0.0612 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97793\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 21s - loss: 0.0961 - accuracy: 0.9694 - val_loss: 0.0614 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97793\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 21s - loss: 0.0951 - accuracy: 0.9700 - val_loss: 0.0592 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97793\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 21s - loss: 0.0865 - accuracy: 0.9720 - val_loss: 0.0598 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97793\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 21s - loss: 0.0909 - accuracy: 0.9709 - val_loss: 0.0587 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97793\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 21s - loss: 0.0884 - accuracy: 0.9725 - val_loss: 0.0595 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97793\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 21s - loss: 0.0870 - accuracy: 0.9715 - val_loss: 0.0590 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97793\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 21s - loss: 0.0898 - accuracy: 0.9732 - val_loss: 0.0591 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97793\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 21s - loss: 0.0840 - accuracy: 0.9726 - val_loss: 0.0589 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97793\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 21s - loss: 0.0804 - accuracy: 0.9732 - val_loss: 0.0610 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97793\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 21s - loss: 0.0795 - accuracy: 0.9749 - val_loss: 0.0607 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97793\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 21s - loss: 0.0863 - accuracy: 0.9712 - val_loss: 0.0606 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97793\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 21s - loss: 0.0898 - accuracy: 0.9714 - val_loss: 0.0595 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97793\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 21s - loss: 0.0830 - accuracy: 0.9734 - val_loss: 0.0599 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97793\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 21s - loss: 0.0857 - accuracy: 0.9722 - val_loss: 0.0599 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97793\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 21s - loss: 0.0838 - accuracy: 0.9727 - val_loss: 0.0595 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97793\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 21s - loss: 0.0847 - accuracy: 0.9730 - val_loss: 0.0598 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97793\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 21s - loss: 0.0787 - accuracy: 0.9753 - val_loss: 0.0597 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97793\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 21s - loss: 0.0855 - accuracy: 0.9730 - val_loss: 0.0595 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97793\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 21s - loss: 0.0766 - accuracy: 0.9761 - val_loss: 0.0586 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97793\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 21s - loss: 0.0842 - accuracy: 0.9735 - val_loss: 0.0597 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97793\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 21s - loss: 0.0785 - accuracy: 0.9757 - val_loss: 0.0584 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97793\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 21s - loss: 0.0776 - accuracy: 0.9756 - val_loss: 0.0595 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97793\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 21s - loss: 0.0792 - accuracy: 0.9736 - val_loss: 0.0594 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97793\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 21s - loss: 0.0736 - accuracy: 0.9770 - val_loss: 0.0605 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97793\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 21s - loss: 0.0795 - accuracy: 0.9751 - val_loss: 0.0597 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97793\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 21s - loss: 0.0776 - accuracy: 0.9746 - val_loss: 0.0595 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97793\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 21s - loss: 0.0846 - accuracy: 0.9725 - val_loss: 0.0601 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97793\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Training_filenames)):\n",
    "    \n",
    "    print(f'Start training the fold{i} !')\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB7(weights='noisy-student', \n",
    "                                include_top=False,\n",
    "                                pooling='avg',\n",
    "                                input_shape=(*IMAGE_SIZE, 3))\n",
    "        model1 = get_model(enet)\n",
    "    \n",
    "\n",
    "    chk_callback1 = tf.keras.callbacks.ModelCheckpoint(f'effnetb7_{i}_best.h5',\n",
    "                                                       save_weights_only=True,\n",
    "#                                                        monitor='val_f1_score',\n",
    "                                                       monitor='val_accuracy',\n",
    "                                                       mode='max',\n",
    "                                                       save_best_only=True,\n",
    "                                                       verbose=1)\n",
    "    \n",
    "    history1 = model1.fit(get_training_dataset(Training_filenames[i], do_onehot=True), \n",
    "                      steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                      epochs=EPOCHS, \n",
    "                      validation_data=get_validation_dataset(Validation_filenames[i], do_onehot=True),\n",
    "                      validation_steps=VALIDATION_STEPS,\n",
    "                      callbacks=[lr_callback, chk_callback1],\n",
    "                      verbose=2)\n",
    "\n",
    "    del chk_callback1, history1, model1\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e5a6c",
   "metadata": {
    "papermill": {
     "duration": 0.269356,
     "end_time": "2021-07-01T10:35:55.284530",
     "exception": false,
     "start_time": "2021-07-01T10:35:55.015174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4f21b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T10:35:55.853786Z",
     "iopub.status.busy": "2021-07-01T10:35:55.848397Z",
     "iopub.status.idle": "2021-07-01T12:19:48.235189Z",
     "shell.execute_reply": "2021-07-01T12:19:48.234408Z"
    },
    "papermill": {
     "duration": 6232.688306,
     "end_time": "2021-07-01T12:19:48.235492",
     "exception": false,
     "start_time": "2021-07-01T10:35:55.547186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the fold0 !\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 377s - loss: 5.0854 - accuracy: 0.0291 - val_loss: 4.7265 - val_accuracy: 0.0651\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.06510, saving model to densenet201_0_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 12s - loss: 3.3242 - accuracy: 0.3554 - val_loss: 2.0975 - val_accuracy: 0.5083\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.06510 to 0.50831, saving model to densenet201_0_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 12s - loss: 1.7223 - accuracy: 0.6602 - val_loss: 1.0225 - val_accuracy: 0.7540\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.50831 to 0.75402, saving model to densenet201_0_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 12s - loss: 0.9362 - accuracy: 0.8088 - val_loss: 0.6304 - val_accuracy: 0.8725\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.75402 to 0.87251, saving model to densenet201_0_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 12s - loss: 0.6046 - accuracy: 0.8705 - val_loss: 0.4087 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.87251 to 0.89458, saving model to densenet201_0_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 12s - loss: 0.4502 - accuracy: 0.8980 - val_loss: 0.6469 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.89458\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 12s - loss: 0.3540 - accuracy: 0.9147 - val_loss: 0.3219 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.89458 to 0.90221, saving model to densenet201_0_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 12s - loss: 0.3132 - accuracy: 0.9245 - val_loss: 0.3275 - val_accuracy: 0.9017\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.90221\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 12s - loss: 0.2932 - accuracy: 0.9255 - val_loss: 0.2833 - val_accuracy: 0.9175\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.90221 to 0.91746, saving model to densenet201_0_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 12s - loss: 0.2721 - accuracy: 0.9286 - val_loss: 0.6444 - val_accuracy: 0.8199\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91746\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 12s - loss: 0.2061 - accuracy: 0.9452 - val_loss: 0.3543 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.91746\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 12s - loss: 0.1779 - accuracy: 0.9528 - val_loss: 0.1701 - val_accuracy: 0.9504\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.91746 to 0.95042, saving model to densenet201_0_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 12s - loss: 0.1473 - accuracy: 0.9603 - val_loss: 0.1375 - val_accuracy: 0.9580\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95042 to 0.95805, saving model to densenet201_0_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 12s - loss: 0.1286 - accuracy: 0.9640 - val_loss: 0.1402 - val_accuracy: 0.9510\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.95805\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 13s - loss: 0.1139 - accuracy: 0.9668 - val_loss: 0.1511 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.95805\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 12s - loss: 0.0997 - accuracy: 0.9704 - val_loss: 0.1242 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.95805\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 12s - loss: 0.0863 - accuracy: 0.9729 - val_loss: 0.1078 - val_accuracy: 0.9640\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.95805 to 0.96404, saving model to densenet201_0_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 12s - loss: 0.0905 - accuracy: 0.9749 - val_loss: 0.0984 - val_accuracy: 0.9668\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.96404 to 0.96677, saving model to densenet201_0_best.h5\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 12s - loss: 0.0775 - accuracy: 0.9763 - val_loss: 0.0790 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.96677 to 0.96895, saving model to densenet201_0_best.h5\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 12s - loss: 0.0783 - accuracy: 0.9766 - val_loss: 0.0895 - val_accuracy: 0.9665\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.96895\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 12s - loss: 0.0744 - accuracy: 0.9774 - val_loss: 0.0898 - val_accuracy: 0.9670\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.96895\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 12s - loss: 0.0711 - accuracy: 0.9782 - val_loss: 0.0774 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.96895 to 0.97004, saving model to densenet201_0_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 12s - loss: 0.0672 - accuracy: 0.9790 - val_loss: 0.0858 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97004 to 0.97085, saving model to densenet201_0_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 12s - loss: 0.0659 - accuracy: 0.9785 - val_loss: 0.0760 - val_accuracy: 0.9709\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97085\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 12s - loss: 0.0650 - accuracy: 0.9794 - val_loss: 0.0767 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.97085\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 12s - loss: 0.0655 - accuracy: 0.9788 - val_loss: 0.0776 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97085 to 0.97303, saving model to densenet201_0_best.h5\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 12s - loss: 0.0582 - accuracy: 0.9817 - val_loss: 0.0771 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.97303 to 0.97439, saving model to densenet201_0_best.h5\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 12s - loss: 0.0578 - accuracy: 0.9805 - val_loss: 0.0736 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97439\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 12s - loss: 0.0509 - accuracy: 0.9831 - val_loss: 0.0708 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.97439 to 0.97467, saving model to densenet201_0_best.h5\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 12s - loss: 0.0561 - accuracy: 0.9816 - val_loss: 0.0729 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.97467 to 0.97494, saving model to densenet201_0_best.h5\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 12s - loss: 0.0535 - accuracy: 0.9823 - val_loss: 0.0737 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.97494 to 0.97576, saving model to densenet201_0_best.h5\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 12s - loss: 0.0499 - accuracy: 0.9820 - val_loss: 0.0716 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97576\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 12s - loss: 0.0507 - accuracy: 0.9831 - val_loss: 0.0730 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97576\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 12s - loss: 0.0438 - accuracy: 0.9851 - val_loss: 0.0704 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97576\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 12s - loss: 0.0495 - accuracy: 0.9837 - val_loss: 0.0714 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97576\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 12s - loss: 0.0460 - accuracy: 0.9842 - val_loss: 0.0734 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97576\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 12s - loss: 0.0418 - accuracy: 0.9864 - val_loss: 0.0712 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97576\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 12s - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.0708 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97576\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 12s - loss: 0.0391 - accuracy: 0.9860 - val_loss: 0.0697 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97576\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 12s - loss: 0.0446 - accuracy: 0.9849 - val_loss: 0.0688 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00040: val_accuracy improved from 0.97576 to 0.97657, saving model to densenet201_0_best.h5\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 12s - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.0709 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97657\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 12s - loss: 0.0441 - accuracy: 0.9853 - val_loss: 0.0707 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97657\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 12s - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.0691 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97657\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 12s - loss: 0.0415 - accuracy: 0.9868 - val_loss: 0.0692 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97657\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 12s - loss: 0.0424 - accuracy: 0.9851 - val_loss: 0.0693 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97657\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 12s - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.0736 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97657\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 12s - loss: 0.0421 - accuracy: 0.9860 - val_loss: 0.0726 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97657\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 12s - loss: 0.0430 - accuracy: 0.9851 - val_loss: 0.0714 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97657\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 12s - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.0716 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97657\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 12s - loss: 0.0388 - accuracy: 0.9866 - val_loss: 0.0734 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97657\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 12s - loss: 0.0419 - accuracy: 0.9856 - val_loss: 0.0754 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97657\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 12s - loss: 0.0397 - accuracy: 0.9866 - val_loss: 0.0730 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97657\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 12s - loss: 0.0417 - accuracy: 0.9849 - val_loss: 0.0721 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97657\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 12s - loss: 0.0369 - accuracy: 0.9870 - val_loss: 0.0720 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97657\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 12s - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.0719 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97657\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 12s - loss: 0.0400 - accuracy: 0.9853 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97657\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 12s - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.0738 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97657\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 12s - loss: 0.0418 - accuracy: 0.9852 - val_loss: 0.0740 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97657\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 12s - loss: 0.0387 - accuracy: 0.9873 - val_loss: 0.0737 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97657\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 12s - loss: 0.0417 - accuracy: 0.9854 - val_loss: 0.0747 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97657\n",
      "Start training the fold1 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 372s - loss: 5.1062 - accuracy: 0.0340 - val_loss: 4.8133 - val_accuracy: 0.0610\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.06102, saving model to densenet201_1_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 12s - loss: 3.3556 - accuracy: 0.3422 - val_loss: 1.9342 - val_accuracy: 0.5276\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.06102 to 0.52765, saving model to densenet201_1_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 12s - loss: 1.7165 - accuracy: 0.6669 - val_loss: 0.9833 - val_accuracy: 0.7785\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52765 to 0.77853, saving model to densenet201_1_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 12s - loss: 0.9422 - accuracy: 0.8072 - val_loss: 0.7611 - val_accuracy: 0.8208\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.77853 to 0.82076, saving model to densenet201_1_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 12s - loss: 0.6029 - accuracy: 0.8697 - val_loss: 0.3833 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.82076 to 0.90439, saving model to densenet201_1_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 12s - loss: 0.4374 - accuracy: 0.8995 - val_loss: 0.3214 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.90439 to 0.91419, saving model to densenet201_1_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 12s - loss: 0.3673 - accuracy: 0.9112 - val_loss: 0.3172 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.91419\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 12s - loss: 0.3182 - accuracy: 0.9223 - val_loss: 0.2752 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.91419 to 0.92073, saving model to densenet201_1_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 12s - loss: 0.2878 - accuracy: 0.9254 - val_loss: 0.2443 - val_accuracy: 0.9256\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.92073 to 0.92563, saving model to densenet201_1_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 12s - loss: 0.2651 - accuracy: 0.9322 - val_loss: 0.2782 - val_accuracy: 0.9186\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.92563\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 12s - loss: 0.2143 - accuracy: 0.9424 - val_loss: 0.2792 - val_accuracy: 0.9218\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.92563\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 12s - loss: 0.1653 - accuracy: 0.9545 - val_loss: 0.2102 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.92563 to 0.93163, saving model to densenet201_1_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 12s - loss: 0.1534 - accuracy: 0.9576 - val_loss: 0.1323 - val_accuracy: 0.9561\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.93163 to 0.95614, saving model to densenet201_1_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 12s - loss: 0.1324 - accuracy: 0.9635 - val_loss: 0.1258 - val_accuracy: 0.9583\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.95614 to 0.95832, saving model to densenet201_1_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 12s - loss: 0.1153 - accuracy: 0.9692 - val_loss: 0.1080 - val_accuracy: 0.9613\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.95832 to 0.96132, saving model to densenet201_1_best.h5\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 12s - loss: 0.1057 - accuracy: 0.9709 - val_loss: 0.0960 - val_accuracy: 0.9654\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96132 to 0.96540, saving model to densenet201_1_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 12s - loss: 0.0947 - accuracy: 0.9722 - val_loss: 0.0980 - val_accuracy: 0.9659\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96540 to 0.96595, saving model to densenet201_1_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 12s - loss: 0.0891 - accuracy: 0.9739 - val_loss: 0.0869 - val_accuracy: 0.9662\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.96595 to 0.96622, saving model to densenet201_1_best.h5\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 16s - loss: 0.0785 - accuracy: 0.9759 - val_loss: 0.0847 - val_accuracy: 0.9657\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.96622\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 12s - loss: 0.0748 - accuracy: 0.9771 - val_loss: 0.0725 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.96622 to 0.97031, saving model to densenet201_1_best.h5\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 12s - loss: 0.0689 - accuracy: 0.9775 - val_loss: 0.0758 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.97031 to 0.97058, saving model to densenet201_1_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 12s - loss: 0.0675 - accuracy: 0.9791 - val_loss: 0.0695 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.97058 to 0.97358, saving model to densenet201_1_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 12s - loss: 0.0678 - accuracy: 0.9772 - val_loss: 0.0637 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97358 to 0.97548, saving model to densenet201_1_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 12s - loss: 0.0612 - accuracy: 0.9809 - val_loss: 0.0700 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97548\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 12s - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0691 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.97548\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 12s - loss: 0.0605 - accuracy: 0.9807 - val_loss: 0.0705 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97548\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 12s - loss: 0.0553 - accuracy: 0.9810 - val_loss: 0.0656 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97548\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 12s - loss: 0.0553 - accuracy: 0.9820 - val_loss: 0.0728 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97548\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 12s - loss: 0.0524 - accuracy: 0.9829 - val_loss: 0.0669 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.97548 to 0.97603, saving model to densenet201_1_best.h5\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 12s - loss: 0.0552 - accuracy: 0.9815 - val_loss: 0.0673 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97603\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 12s - loss: 0.0469 - accuracy: 0.9840 - val_loss: 0.0649 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00031: val_accuracy improved from 0.97603 to 0.97630, saving model to densenet201_1_best.h5\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 12s - loss: 0.0479 - accuracy: 0.9836 - val_loss: 0.0659 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.97630 to 0.97657, saving model to densenet201_1_best.h5\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 12s - loss: 0.0438 - accuracy: 0.9855 - val_loss: 0.0640 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97657\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 12s - loss: 0.0503 - accuracy: 0.9836 - val_loss: 0.0675 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97657\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 12s - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.0665 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.97657 to 0.97766, saving model to densenet201_1_best.h5\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 12s - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.0649 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00036: val_accuracy improved from 0.97766 to 0.97794, saving model to densenet201_1_best.h5\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 12s - loss: 0.0491 - accuracy: 0.9836 - val_loss: 0.0644 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.97794 to 0.97848, saving model to densenet201_1_best.h5\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 12s - loss: 0.0434 - accuracy: 0.9851 - val_loss: 0.0656 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97848\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 12s - loss: 0.0482 - accuracy: 0.9842 - val_loss: 0.0631 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97848\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 12s - loss: 0.0471 - accuracy: 0.9834 - val_loss: 0.0600 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97848\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 12s - loss: 0.0367 - accuracy: 0.9871 - val_loss: 0.0627 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97848\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 12s - loss: 0.0426 - accuracy: 0.9855 - val_loss: 0.0641 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97848\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 12s - loss: 0.0425 - accuracy: 0.9851 - val_loss: 0.0658 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97848\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 12s - loss: 0.0457 - accuracy: 0.9849 - val_loss: 0.0631 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97848\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 12s - loss: 0.0412 - accuracy: 0.9858 - val_loss: 0.0634 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97848\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 12s - loss: 0.0417 - accuracy: 0.9846 - val_loss: 0.0634 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97848\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 12s - loss: 0.0402 - accuracy: 0.9860 - val_loss: 0.0648 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97848\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 12s - loss: 0.0372 - accuracy: 0.9877 - val_loss: 0.0649 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97848\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 12s - loss: 0.0431 - accuracy: 0.9851 - val_loss: 0.0660 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97848\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 12s - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.0641 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97848\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 12s - loss: 0.0397 - accuracy: 0.9853 - val_loss: 0.0649 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97848\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 12s - loss: 0.0375 - accuracy: 0.9871 - val_loss: 0.0651 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97848\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 12s - loss: 0.0395 - accuracy: 0.9851 - val_loss: 0.0653 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97848\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 12s - loss: 0.0402 - accuracy: 0.9849 - val_loss: 0.0669 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97848\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 12s - loss: 0.0389 - accuracy: 0.9855 - val_loss: 0.0655 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97848\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 12s - loss: 0.0363 - accuracy: 0.9861 - val_loss: 0.0687 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97848\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 12s - loss: 0.0368 - accuracy: 0.9865 - val_loss: 0.0694 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97848\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 12s - loss: 0.0394 - accuracy: 0.9867 - val_loss: 0.0666 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97848\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 12s - loss: 0.0379 - accuracy: 0.9866 - val_loss: 0.0675 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97848\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 12s - loss: 0.0391 - accuracy: 0.9853 - val_loss: 0.0678 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97848\n",
      "Start training the fold2 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 376s - loss: 5.0959 - accuracy: 0.0310 - val_loss: 4.8373 - val_accuracy: 0.0488\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.04876, saving model to densenet201_2_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 12s - loss: 3.3368 - accuracy: 0.3587 - val_loss: 1.9926 - val_accuracy: 0.5448\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.04876 to 0.54481, saving model to densenet201_2_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 12s - loss: 1.6992 - accuracy: 0.6728 - val_loss: 0.9840 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.54481 to 0.77363, saving model to densenet201_2_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 12s - loss: 0.9389 - accuracy: 0.8100 - val_loss: 0.5529 - val_accuracy: 0.8736\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.77363 to 0.87360, saving model to densenet201_2_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 12s - loss: 0.5910 - accuracy: 0.8733 - val_loss: 0.4300 - val_accuracy: 0.8921\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.87360 to 0.89213, saving model to densenet201_2_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 12s - loss: 0.4458 - accuracy: 0.8981 - val_loss: 0.4089 - val_accuracy: 0.8995\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.89213 to 0.89948, saving model to densenet201_2_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 12s - loss: 0.3596 - accuracy: 0.9152 - val_loss: 0.3384 - val_accuracy: 0.9036\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.89948 to 0.90357, saving model to densenet201_2_best.h5\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 12s - loss: 0.3119 - accuracy: 0.9245 - val_loss: 0.3085 - val_accuracy: 0.9057\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.90357 to 0.90575, saving model to densenet201_2_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 12s - loss: 0.2706 - accuracy: 0.9311 - val_loss: 0.2870 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.90575 to 0.91392, saving model to densenet201_2_best.h5\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 12s - loss: 0.2782 - accuracy: 0.9274 - val_loss: 0.3775 - val_accuracy: 0.8845\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91392\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 12s - loss: 0.2009 - accuracy: 0.9496 - val_loss: 0.2280 - val_accuracy: 0.9267\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.91392 to 0.92672, saving model to densenet201_2_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 12s - loss: 0.1736 - accuracy: 0.9531 - val_loss: 0.1445 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.92672 to 0.95342, saving model to densenet201_2_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 12s - loss: 0.1344 - accuracy: 0.9633 - val_loss: 0.1261 - val_accuracy: 0.9621\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.95342 to 0.96214, saving model to densenet201_2_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 12s - loss: 0.1251 - accuracy: 0.9651 - val_loss: 0.1670 - val_accuracy: 0.9458\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.96214\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 12s - loss: 0.1062 - accuracy: 0.9685 - val_loss: 0.1256 - val_accuracy: 0.9586\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.96214\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 12s - loss: 0.0950 - accuracy: 0.9731 - val_loss: 0.1067 - val_accuracy: 0.9640\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96214 to 0.96404, saving model to densenet201_2_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 12s - loss: 0.0937 - accuracy: 0.9727 - val_loss: 0.4702 - val_accuracy: 0.8643\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.96404\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 12s - loss: 0.0897 - accuracy: 0.9736 - val_loss: 0.1227 - val_accuracy: 0.9570\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.96404\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 16s - loss: 0.0878 - accuracy: 0.9733 - val_loss: 0.0956 - val_accuracy: 0.9621\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.96404\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 12s - loss: 0.0785 - accuracy: 0.9764 - val_loss: 0.0863 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.96404 to 0.96922, saving model to densenet201_2_best.h5\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 12s - loss: 0.0716 - accuracy: 0.9771 - val_loss: 0.0855 - val_accuracy: 0.9684\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.96922\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 12s - loss: 0.0672 - accuracy: 0.9777 - val_loss: 0.0855 - val_accuracy: 0.9684\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.96922\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 12s - loss: 0.0671 - accuracy: 0.9794 - val_loss: 0.0808 - val_accuracy: 0.9706\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.96922 to 0.97058, saving model to densenet201_2_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 12s - loss: 0.0562 - accuracy: 0.9820 - val_loss: 0.0962 - val_accuracy: 0.9651\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97058\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 12s - loss: 0.0587 - accuracy: 0.9802 - val_loss: 0.0709 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97058 to 0.97194, saving model to densenet201_2_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 12s - loss: 0.0578 - accuracy: 0.9811 - val_loss: 0.0760 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97194 to 0.97276, saving model to densenet201_2_best.h5\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 12s - loss: 0.0584 - accuracy: 0.9803 - val_loss: 0.0776 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97276\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 12s - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0760 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97276\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 12s - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.0793 - val_accuracy: 0.9700\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97276\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 12s - loss: 0.0478 - accuracy: 0.9851 - val_loss: 0.0760 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.97276 to 0.97303, saving model to densenet201_2_best.h5\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 12s - loss: 0.0478 - accuracy: 0.9836 - val_loss: 0.0772 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97303\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 12s - loss: 0.0530 - accuracy: 0.9820 - val_loss: 0.0752 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97303\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 12s - loss: 0.0459 - accuracy: 0.9848 - val_loss: 0.0735 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00033: val_accuracy improved from 0.97303 to 0.97385, saving model to densenet201_2_best.h5\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 12s - loss: 0.0478 - accuracy: 0.9840 - val_loss: 0.0785 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97385\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 12s - loss: 0.0478 - accuracy: 0.9840 - val_loss: 0.0848 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97385\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 12s - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.0815 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97385\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 12s - loss: 0.0460 - accuracy: 0.9837 - val_loss: 0.0837 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97385\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 12s - loss: 0.0468 - accuracy: 0.9849 - val_loss: 0.0789 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97385\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 12s - loss: 0.0466 - accuracy: 0.9836 - val_loss: 0.0775 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97385\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 12s - loss: 0.0437 - accuracy: 0.9845 - val_loss: 0.0804 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97385\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 12s - loss: 0.0417 - accuracy: 0.9860 - val_loss: 0.0791 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97385\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 12s - loss: 0.0441 - accuracy: 0.9845 - val_loss: 0.0782 - val_accuracy: 0.9719\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97385\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 12s - loss: 0.0395 - accuracy: 0.9869 - val_loss: 0.0740 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97385\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 12s - loss: 0.0399 - accuracy: 0.9866 - val_loss: 0.0757 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00044: val_accuracy improved from 0.97385 to 0.97412, saving model to densenet201_2_best.h5\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 12s - loss: 0.0402 - accuracy: 0.9864 - val_loss: 0.0766 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97412\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 12s - loss: 0.0407 - accuracy: 0.9852 - val_loss: 0.0787 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97412\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 12s - loss: 0.0414 - accuracy: 0.9855 - val_loss: 0.0777 - val_accuracy: 0.9703\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97412\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 12s - loss: 0.0365 - accuracy: 0.9870 - val_loss: 0.0803 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97412\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 12s - loss: 0.0392 - accuracy: 0.9858 - val_loss: 0.0783 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97412\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 12s - loss: 0.0368 - accuracy: 0.9870 - val_loss: 0.0746 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97412\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 12s - loss: 0.0427 - accuracy: 0.9846 - val_loss: 0.0757 - val_accuracy: 0.9728\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97412\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 12s - loss: 0.0385 - accuracy: 0.9868 - val_loss: 0.0751 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00052: val_accuracy improved from 0.97412 to 0.97439, saving model to densenet201_2_best.h5\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 12s - loss: 0.0341 - accuracy: 0.9879 - val_loss: 0.0777 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97439\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 12s - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.0788 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97439\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 12s - loss: 0.0389 - accuracy: 0.9862 - val_loss: 0.0788 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97439\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 12s - loss: 0.0390 - accuracy: 0.9872 - val_loss: 0.0789 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97439\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 12s - loss: 0.0364 - accuracy: 0.9871 - val_loss: 0.0783 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97439\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 12s - loss: 0.0375 - accuracy: 0.9860 - val_loss: 0.0782 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97439\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 12s - loss: 0.0369 - accuracy: 0.9879 - val_loss: 0.0783 - val_accuracy: 0.9736\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97439\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 12s - loss: 0.0368 - accuracy: 0.9863 - val_loss: 0.0771 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97439\n",
      "Start training the fold3 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 383s - loss: 5.1178 - accuracy: 0.0304 - val_loss: 4.8335 - val_accuracy: 0.0638\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.06376, saving model to densenet201_3_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 12s - loss: 3.3718 - accuracy: 0.3458 - val_loss: 2.0777 - val_accuracy: 0.5174\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.06376 to 0.51744, saving model to densenet201_3_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 12s - loss: 1.7496 - accuracy: 0.6514 - val_loss: 1.0257 - val_accuracy: 0.7599\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.51744 to 0.75995, saving model to densenet201_3_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 12s - loss: 0.9526 - accuracy: 0.8046 - val_loss: 0.5902 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.75995 to 0.85586, saving model to densenet201_3_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 12s - loss: 0.6051 - accuracy: 0.8673 - val_loss: 0.3855 - val_accuracy: 0.9052\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.85586 to 0.90518, saving model to densenet201_3_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 12s - loss: 0.4465 - accuracy: 0.8956 - val_loss: 0.3177 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.90518 to 0.92044, saving model to densenet201_3_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 12s - loss: 0.3527 - accuracy: 0.9159 - val_loss: 0.3113 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.92044\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 12s - loss: 0.3278 - accuracy: 0.9184 - val_loss: 0.2607 - val_accuracy: 0.9215\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.92044 to 0.92153, saving model to densenet201_3_best.h5\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 12s - loss: 0.2885 - accuracy: 0.9265 - val_loss: 0.3743 - val_accuracy: 0.8984\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.92153\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 12s - loss: 0.2670 - accuracy: 0.9293 - val_loss: 0.3695 - val_accuracy: 0.8940\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.92153\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 12s - loss: 0.2217 - accuracy: 0.9404 - val_loss: 0.2069 - val_accuracy: 0.9354\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.92153 to 0.93542, saving model to densenet201_3_best.h5\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 12s - loss: 0.1808 - accuracy: 0.9493 - val_loss: 0.2206 - val_accuracy: 0.9281\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.93542\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 12s - loss: 0.1456 - accuracy: 0.9603 - val_loss: 0.1786 - val_accuracy: 0.9398\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.93542 to 0.93978, saving model to densenet201_3_best.h5\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 12s - loss: 0.1360 - accuracy: 0.9624 - val_loss: 0.1120 - val_accuracy: 0.9610\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.93978 to 0.96104, saving model to densenet201_3_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 12s - loss: 0.1112 - accuracy: 0.9676 - val_loss: 0.1335 - val_accuracy: 0.9550\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.96104\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 12s - loss: 0.1025 - accuracy: 0.9701 - val_loss: 0.1191 - val_accuracy: 0.9608\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.96104\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 12s - loss: 0.0956 - accuracy: 0.9707 - val_loss: 0.0808 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96104 to 0.96921, saving model to densenet201_3_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 12s - loss: 0.0864 - accuracy: 0.9736 - val_loss: 0.1156 - val_accuracy: 0.9610\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.96921\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 16s - loss: 0.0820 - accuracy: 0.9758 - val_loss: 0.0988 - val_accuracy: 0.9610\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.96921\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 12s - loss: 0.0760 - accuracy: 0.9764 - val_loss: 0.1119 - val_accuracy: 0.9624\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.96921\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 12s - loss: 0.0732 - accuracy: 0.9772 - val_loss: 0.0770 - val_accuracy: 0.9695\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.96921 to 0.96948, saving model to densenet201_3_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 12s - loss: 0.0681 - accuracy: 0.9788 - val_loss: 0.0843 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.96948 to 0.97248, saving model to densenet201_3_best.h5\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 12s - loss: 0.0711 - accuracy: 0.9781 - val_loss: 0.0778 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.97248\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 12s - loss: 0.0618 - accuracy: 0.9801 - val_loss: 0.0723 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.97248 to 0.97302, saving model to densenet201_3_best.h5\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 12s - loss: 0.0554 - accuracy: 0.9825 - val_loss: 0.0719 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97302 to 0.97602, saving model to densenet201_3_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 12s - loss: 0.0611 - accuracy: 0.9803 - val_loss: 0.0664 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97602 to 0.97629, saving model to densenet201_3_best.h5\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 12s - loss: 0.0594 - accuracy: 0.9804 - val_loss: 0.0685 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.97629 to 0.97847, saving model to densenet201_3_best.h5\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 12s - loss: 0.0531 - accuracy: 0.9820 - val_loss: 0.0654 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97847\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 12s - loss: 0.0522 - accuracy: 0.9825 - val_loss: 0.0692 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97847\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 12s - loss: 0.0503 - accuracy: 0.9829 - val_loss: 0.0679 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97847\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 12s - loss: 0.0530 - accuracy: 0.9827 - val_loss: 0.0626 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97847\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 12s - loss: 0.0472 - accuracy: 0.9845 - val_loss: 0.0655 - val_accuracy: 0.9733\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97847\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 12s - loss: 0.0480 - accuracy: 0.9831 - val_loss: 0.0633 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97847\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 12s - loss: 0.0464 - accuracy: 0.9840 - val_loss: 0.0632 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97847\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 12s - loss: 0.0468 - accuracy: 0.9836 - val_loss: 0.0644 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.97847\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 12s - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.0641 - val_accuracy: 0.9779\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97847\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 12s - loss: 0.0477 - accuracy: 0.9829 - val_loss: 0.0658 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.97847\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 12s - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.0610 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97847\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 12s - loss: 0.0471 - accuracy: 0.9840 - val_loss: 0.0613 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97847\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 12s - loss: 0.0482 - accuracy: 0.9827 - val_loss: 0.0603 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97847\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 12s - loss: 0.0422 - accuracy: 0.9851 - val_loss: 0.0609 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97847\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 12s - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.0605 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97847\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 12s - loss: 0.0403 - accuracy: 0.9859 - val_loss: 0.0608 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97847\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 12s - loss: 0.0432 - accuracy: 0.9840 - val_loss: 0.0630 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97847\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 12s - loss: 0.0425 - accuracy: 0.9839 - val_loss: 0.0638 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97847\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 12s - loss: 0.0409 - accuracy: 0.9846 - val_loss: 0.0626 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97847\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 12s - loss: 0.0454 - accuracy: 0.9829 - val_loss: 0.0603 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97847\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 12s - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.0606 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97847\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 12s - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.0638 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97847\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 12s - loss: 0.0388 - accuracy: 0.9872 - val_loss: 0.0639 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97847\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 12s - loss: 0.0406 - accuracy: 0.9848 - val_loss: 0.0644 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97847\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 12s - loss: 0.0406 - accuracy: 0.9852 - val_loss: 0.0639 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97847\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 12s - loss: 0.0346 - accuracy: 0.9869 - val_loss: 0.0632 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97847\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 12s - loss: 0.0368 - accuracy: 0.9864 - val_loss: 0.0631 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97847\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 12s - loss: 0.0431 - accuracy: 0.9853 - val_loss: 0.0639 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97847\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 12s - loss: 0.0407 - accuracy: 0.9854 - val_loss: 0.0639 - val_accuracy: 0.9760\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97847\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 12s - loss: 0.0443 - accuracy: 0.9842 - val_loss: 0.0638 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97847\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 12s - loss: 0.0369 - accuracy: 0.9864 - val_loss: 0.0629 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97847\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 12s - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.0616 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97847\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 12s - loss: 0.0373 - accuracy: 0.9860 - val_loss: 0.0602 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97847\n",
      "Start training the fold4 !\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "114/114 - 376s - loss: 5.1077 - accuracy: 0.0339 - val_loss: 4.8337 - val_accuracy: 0.0561\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.05613, saving model to densenet201_4_best.h5\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 5.333333333333333e-05.\n",
      "114/114 - 12s - loss: 3.3351 - accuracy: 0.3490 - val_loss: 2.0293 - val_accuracy: 0.5281\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.05613 to 0.52807, saving model to densenet201_4_best.h5\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.666666666666667e-05.\n",
      "114/114 - 12s - loss: 1.7070 - accuracy: 0.6656 - val_loss: 0.9596 - val_accuracy: 0.7782\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52807 to 0.77820, saving model to densenet201_4_best.h5\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00014000000000000001.\n",
      "114/114 - 12s - loss: 0.9314 - accuracy: 0.8130 - val_loss: 0.5647 - val_accuracy: 0.8597\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.77820 to 0.85967, saving model to densenet201_4_best.h5\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.00018333333333333334.\n",
      "114/114 - 12s - loss: 0.5882 - accuracy: 0.8710 - val_loss: 0.4146 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.85967 to 0.90218, saving model to densenet201_4_best.h5\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00022666666666666666.\n",
      "114/114 - 12s - loss: 0.4390 - accuracy: 0.8989 - val_loss: 0.3093 - val_accuracy: 0.9166\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.90218 to 0.91662, saving model to densenet201_4_best.h5\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00027000000000000006.\n",
      "114/114 - 12s - loss: 0.3551 - accuracy: 0.9130 - val_loss: 0.3356 - val_accuracy: 0.9095\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.91662\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003133333333333334.\n",
      "114/114 - 12s - loss: 0.3121 - accuracy: 0.9239 - val_loss: 0.3720 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.91662\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0003566666666666667.\n",
      "114/114 - 12s - loss: 0.2992 - accuracy: 0.9229 - val_loss: 0.3753 - val_accuracy: 0.8956\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.91662\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "114/114 - 12s - loss: 0.2665 - accuracy: 0.9304 - val_loss: 0.3292 - val_accuracy: 0.9025\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.91662\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00034930000000000003.\n",
      "114/114 - 12s - loss: 0.2066 - accuracy: 0.9438 - val_loss: 0.3474 - val_accuracy: 0.8973\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.91662\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000305191.\n",
      "114/114 - 12s - loss: 0.1677 - accuracy: 0.9536 - val_loss: 0.1921 - val_accuracy: 0.9411\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.91662 to 0.94114, saving model to densenet201_4_best.h5\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026681617.\n",
      "114/114 - 12s - loss: 0.1397 - accuracy: 0.9626 - val_loss: 0.2179 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.94114\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0002334300679.\n",
      "114/114 - 12s - loss: 0.1277 - accuracy: 0.9631 - val_loss: 0.1242 - val_accuracy: 0.9594\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.94114 to 0.95940, saving model to densenet201_4_best.h5\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.000204384159073.\n",
      "114/114 - 12s - loss: 0.1074 - accuracy: 0.9688 - val_loss: 0.1191 - val_accuracy: 0.9624\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.95940 to 0.96240, saving model to densenet201_4_best.h5\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00017911421839351.\n",
      "114/114 - 12s - loss: 0.0978 - accuracy: 0.9714 - val_loss: 0.1031 - val_accuracy: 0.9646\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.96240 to 0.96458, saving model to densenet201_4_best.h5\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001571293700023537.\n",
      "114/114 - 12s - loss: 0.0979 - accuracy: 0.9723 - val_loss: 0.0886 - val_accuracy: 0.9657\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.96458 to 0.96567, saving model to densenet201_4_best.h5\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00013800255190204772.\n",
      "114/114 - 12s - loss: 0.0870 - accuracy: 0.9744 - val_loss: 0.0895 - val_accuracy: 0.9670\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.96567 to 0.96703, saving model to densenet201_4_best.h5\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00012136222015478151.\n",
      "114/114 - 15s - loss: 0.0807 - accuracy: 0.9750 - val_loss: 0.0887 - val_accuracy: 0.9708\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.96703 to 0.97084, saving model to densenet201_4_best.h5\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00010688513153465992.\n",
      "114/114 - 12s - loss: 0.0739 - accuracy: 0.9765 - val_loss: 0.0795 - val_accuracy: 0.9711\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.97084 to 0.97112, saving model to densenet201_4_best.h5\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.429006443515412e-05.\n",
      "114/114 - 12s - loss: 0.0752 - accuracy: 0.9767 - val_loss: 0.0774 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.97112 to 0.97139, saving model to densenet201_4_best.h5\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 8.333235605858409e-05.\n",
      "114/114 - 12s - loss: 0.0650 - accuracy: 0.9790 - val_loss: 0.0756 - val_accuracy: 0.9714\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.97139\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 7.379914977096815e-05.\n",
      "114/114 - 12s - loss: 0.0684 - accuracy: 0.9781 - val_loss: 0.0802 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.97139 to 0.97302, saving model to densenet201_4_best.h5\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 6.55052603007423e-05.\n",
      "114/114 - 12s - loss: 0.0632 - accuracy: 0.9796 - val_loss: 0.0967 - val_accuracy: 0.9681\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97302\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 5.828957646164579e-05.\n",
      "114/114 - 12s - loss: 0.0608 - accuracy: 0.9805 - val_loss: 0.0770 - val_accuracy: 0.9738\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97302 to 0.97384, saving model to densenet201_4_best.h5\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 5.2011931521631845e-05.\n",
      "114/114 - 12s - loss: 0.0623 - accuracy: 0.9785 - val_loss: 0.0764 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.97384\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 4.6550380423819704e-05.\n",
      "114/114 - 12s - loss: 0.0571 - accuracy: 0.9806 - val_loss: 0.0758 - val_accuracy: 0.9741\n",
      "\n",
      "Epoch 00027: val_accuracy improved from 0.97384 to 0.97411, saving model to densenet201_4_best.h5\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 4.179883096872314e-05.\n",
      "114/114 - 12s - loss: 0.0598 - accuracy: 0.9800 - val_loss: 0.0768 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97411\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.766498294278913e-05.\n",
      "114/114 - 12s - loss: 0.0557 - accuracy: 0.9814 - val_loss: 0.0690 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.97411 to 0.97439, saving model to densenet201_4_best.h5\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.4068535160226545e-05.\n",
      "114/114 - 12s - loss: 0.0483 - accuracy: 0.9847 - val_loss: 0.0728 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.97439 to 0.97657, saving model to densenet201_4_best.h5\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.0939625589397095e-05.\n",
      "114/114 - 12s - loss: 0.0492 - accuracy: 0.9833 - val_loss: 0.0756 - val_accuracy: 0.9747\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.97657\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2.8217474262775473e-05.\n",
      "114/114 - 12s - loss: 0.0530 - accuracy: 0.9814 - val_loss: 0.0710 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.97657\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2.5849202608614662e-05.\n",
      "114/114 - 12s - loss: 0.0526 - accuracy: 0.9819 - val_loss: 0.0784 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.97657\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.3788806269494755e-05.\n",
      "114/114 - 12s - loss: 0.0489 - accuracy: 0.9834 - val_loss: 0.0722 - val_accuracy: 0.9763\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.97657\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.1996261454460435e-05.\n",
      "114/114 - 12s - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.0695 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.97657 to 0.97684, saving model to densenet201_4_best.h5\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.043674746538058e-05.\n",
      "114/114 - 12s - loss: 0.0462 - accuracy: 0.9840 - val_loss: 0.0707 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.97684\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.9079970294881105e-05.\n",
      "114/114 - 12s - loss: 0.0484 - accuracy: 0.9833 - val_loss: 0.0737 - val_accuracy: 0.9782\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.97684 to 0.97820, saving model to densenet201_4_best.h5\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.7899574156546562e-05.\n",
      "114/114 - 12s - loss: 0.0448 - accuracy: 0.9842 - val_loss: 0.0696 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.97820\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.687262951619551e-05.\n",
      "114/114 - 12s - loss: 0.0423 - accuracy: 0.9850 - val_loss: 0.0723 - val_accuracy: 0.9777\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.97820\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.5979187679090094e-05.\n",
      "114/114 - 12s - loss: 0.0418 - accuracy: 0.9849 - val_loss: 0.0721 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.97820\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1.5201893280808381e-05.\n",
      "114/114 - 12s - loss: 0.0460 - accuracy: 0.9840 - val_loss: 0.0750 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.97820\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1.4525647154303291e-05.\n",
      "114/114 - 12s - loss: 0.0455 - accuracy: 0.9839 - val_loss: 0.0726 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.97820\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1.3937313024243864e-05.\n",
      "114/114 - 12s - loss: 0.0423 - accuracy: 0.9858 - val_loss: 0.0713 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.97820\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1.342546233109216e-05.\n",
      "114/114 - 12s - loss: 0.0392 - accuracy: 0.9856 - val_loss: 0.0757 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.97820\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1.298015222805018e-05.\n",
      "114/114 - 12s - loss: 0.0400 - accuracy: 0.9864 - val_loss: 0.0737 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.97820\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1.2592732438403657e-05.\n",
      "114/114 - 12s - loss: 0.0395 - accuracy: 0.9866 - val_loss: 0.0746 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.97820\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1.2255677221411182e-05.\n",
      "114/114 - 12s - loss: 0.0431 - accuracy: 0.9857 - val_loss: 0.0750 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.97820\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1.1962439182627728e-05.\n",
      "114/114 - 12s - loss: 0.0439 - accuracy: 0.9848 - val_loss: 0.0742 - val_accuracy: 0.9768\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.97820\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1.1707322088886124e-05.\n",
      "114/114 - 12s - loss: 0.0363 - accuracy: 0.9872 - val_loss: 0.0750 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.97820\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1485370217330927e-05.\n",
      "114/114 - 12s - loss: 0.0404 - accuracy: 0.9855 - val_loss: 0.0737 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.97820\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.1292272089077907e-05.\n",
      "114/114 - 12s - loss: 0.0389 - accuracy: 0.9857 - val_loss: 0.0732 - val_accuracy: 0.9771\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.97820\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.112427671749778e-05.\n",
      "114/114 - 12s - loss: 0.0412 - accuracy: 0.9845 - val_loss: 0.0744 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.97820\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.0978120744223069e-05.\n",
      "114/114 - 12s - loss: 0.0366 - accuracy: 0.9867 - val_loss: 0.0768 - val_accuracy: 0.9757\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.97820\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.085096504747407e-05.\n",
      "114/114 - 12s - loss: 0.0340 - accuracy: 0.9876 - val_loss: 0.0750 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.97820\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.0740339591302441e-05.\n",
      "114/114 - 12s - loss: 0.0412 - accuracy: 0.9849 - val_loss: 0.0752 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.97820\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.0644095444433124e-05.\n",
      "114/114 - 12s - loss: 0.0402 - accuracy: 0.9865 - val_loss: 0.0716 - val_accuracy: 0.9749\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.97820\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.0560363036656818e-05.\n",
      "114/114 - 12s - loss: 0.0420 - accuracy: 0.9840 - val_loss: 0.0734 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.97820\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.0487515841891432e-05.\n",
      "114/114 - 12s - loss: 0.0385 - accuracy: 0.9855 - val_loss: 0.0790 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.97820\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.0424138782445545e-05.\n",
      "114/114 - 12s - loss: 0.0394 - accuracy: 0.9866 - val_loss: 0.0729 - val_accuracy: 0.9755\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.97820\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.0369000740727624e-05.\n",
      "114/114 - 12s - loss: 0.0352 - accuracy: 0.9875 - val_loss: 0.0771 - val_accuracy: 0.9766\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.97820\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Training_filenames)):\n",
    "    \n",
    "    print(f'Start training the fold{i} !')\n",
    "    with strategy.scope():\n",
    "        densenet = tf.keras.applications.DenseNet201(weights='imagenet', \n",
    "                                                    include_top=False,\n",
    "                                                    pooling='avg',\n",
    "                                                    input_shape=(*IMAGE_SIZE, 3))\n",
    "        model2 = get_model(densenet)\n",
    "\n",
    "    chk_callback2 = tf.keras.callbacks.ModelCheckpoint(f'densenet201_{i}_best.h5',\n",
    "                                                       save_weights_only=True,\n",
    "#                                                        monitor='val_f1_score',\n",
    "                                                       monitor='val_accuracy',\n",
    "                                                       mode='max',\n",
    "                                                       save_best_only=True,\n",
    "                                                       verbose=1)\n",
    "    \n",
    "    history2 = model2.fit(get_training_dataset(Training_filenames[i], do_onehot=True), \n",
    "                      steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                      epochs=EPOCHS, \n",
    "                      validation_data=get_validation_dataset(Validation_filenames[i], do_onehot=True),\n",
    "                      validation_steps=VALIDATION_STEPS,\n",
    "                      callbacks=[lr_callback, chk_callback2],\n",
    "                      verbose=2)\n",
    "\n",
    "    del chk_callback2, history2, model2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28618d5",
   "metadata": {
    "papermill": {
     "duration": 0.48572,
     "end_time": "2021-07-01T12:19:49.201521",
     "exception": false,
     "start_time": "2021-07-01T12:19:48.715801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec3fb62e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T12:19:50.196464Z",
     "iopub.status.busy": "2021-07-01T12:19:50.195536Z",
     "iopub.status.idle": "2021-07-01T12:19:50.199162Z",
     "shell.execute_reply": "2021-07-01T12:19:50.198515Z"
    },
    "papermill": {
     "duration": 0.506434,
     "end_time": "2021-07-01T12:19:50.199304",
     "exception": false,
     "start_time": "2021-07-01T12:19:49.692870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(modellist, model):\n",
    "    test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "    test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "    predslist = []\n",
    "    for modelpath in modellist:\n",
    "        model.load_weights(modelpath)\n",
    "        predslist.append(model.predict(test_images_ds,verbose=0, steps=TEST_STEPS))\n",
    "    \n",
    "    preds = np.sum(predslist, axis = 0)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e923a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T12:19:51.193069Z",
     "iopub.status.busy": "2021-07-01T12:19:51.192199Z",
     "iopub.status.idle": "2021-07-01T12:19:51.276113Z",
     "shell.execute_reply": "2021-07-01T12:19:51.275496Z"
    },
    "papermill": {
     "duration": 0.58233,
     "end_time": "2021-07-01T12:19:51.276258",
     "exception": false,
     "start_time": "2021-07-01T12:19:50.693928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/classify-leaves/train.csv')\n",
    "train.head()\n",
    "leaves_labels = sorted(list(set(train['label'])))\n",
    "n_classes = len(leaves_labels)\n",
    "class_to_num = dict(zip(leaves_labels, range(n_classes)))\n",
    "num_to_class = {v : k for k, v in class_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5287f31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T12:19:52.247883Z",
     "iopub.status.busy": "2021-07-01T12:19:52.246820Z",
     "iopub.status.idle": "2021-07-01T12:50:04.413464Z",
     "shell.execute_reply": "2021-07-01T12:50:04.414011Z"
    },
    "papermill": {
     "duration": 1812.657918,
     "end_time": "2021-07-01T12:50:04.414214",
     "exception": false,
     "start_time": "2021-07-01T12:19:51.756296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions...\n",
      "Generating submission file...\n"
     ]
    }
   ],
   "source": [
    "test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "\n",
    "print('Calculating predictions...')\n",
    "model1list = [f'./effnetb7_{i}_best.h5' for i in range(len(Training_filenames))]\n",
    "model2list = [f'./densenet201_{i}_best.h5' for i in range(len(Training_filenames))]\n",
    "model1 = get_model(enet)\n",
    "model2 = get_model(densenet)\n",
    "probs1 = run_inference(model1list,model1)\n",
    "probs2 = run_inference(model2list,model2)\n",
    "probabilities = 0.5*probs1 + 0.5*probs2\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "preds = []\n",
    "for i in predictions:\n",
    "    preds.append(num_to_class[i])\n",
    "\n",
    "print('Generating submission file...')\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "np.savetxt('submission.csv', np.rec.fromarrays([test_ids, preds]), fmt=['%s', '%s'], delimiter=',', header='image,label', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17199.935578,
   "end_time": "2021-07-01T12:50:07.605224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-01T08:03:27.669646",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
